Отлично! Рефакторинг — это хорошая практика для улучшения читаемости, поддержки и структуры кода. Я провел полный рефакторинг предоставленного кода, стараясь улучшить организацию, читаемость и следовать некоторым лучшим практикам, сохраняя при этом исходный функционал.

Основные изменения:

1.  **Структура `MainWindow`:**
    *   Инициализация (`__init__`) разделена на логические блоки: `_init_state`, `_load_config`, `_init_ui`, `_connect_signals`.
    *   Создание виджетов для вкладок и панели параметров вынесено в отдельные методы.
    *   Общие элементы UI (выбор источника, поле для пути к файлу) создаются с помощью вспомогательных методов (`_create_source_selector_row`, `_create_file_path_row`), чтобы избежать дублирования кода.
2.  **Управление состоянием:** Добавлены флаги (`_is_calibrating`, `_is_working`) для отслеживания текущего состояния приложения. Используются флаги остановки потоков (`_calib_stop_flag`, `_working_stop_flag`).
3.  **Обработка параметров:** Логика загрузки/сохранения параметров вынесена в отдельные методы. Инициализация `self.detection_params` происходит перед использованием. При загрузке используются значения по умолчанию, если файл отсутствует или не содержит нужных ключей.
4.  **Работа с потоками:** Явно используются `threading.Thread`. Добавлена проверка `is_alive()` и попытка `join()` для потоков в `closeEvent` для более чистого завершения (хотя `daemon=True` обычно достаточно).
5.  **Обработка видео:** Функция `frame_to_qpixmap` сделана статическим методом `Utils.frame_to_qpixmap` для переиспользования и вынесена во вспомогательный класс `Utils`.
6.  **Рабочая область:** Логика подготовки рабочей области (`_prepare_working_area`) выполняется синхронно в основном потоке *перед* запуском рабочего потока, чтобы избежать проблем с GUI-взаимодействием из фонового потока. Метод подтверждения (`confirm_working_area`) оставлен без изменений, так как `QEventLoop` — рабочий способ блокировки *основного потока* для ожидания пользовательского ввода в данном контексте.
7.  **Читаемость:** Добавлены комментарии, улучшены имена переменных, код отформатирован для лучшей читаемости. Импорты сгруппированы.
8.  **Класс `Utils`:** Добавлен небольшой вспомогательный класс для общих утилит (пока только `frame_to_qpixmap` и `toggle_layout_visibility`).
9.  **Модули (`ultimate.py`, `calib.py`, `robot_comm.py`, `variables.py`):**
    *   В `ultimate.py`: Небольшие улучшения читаемости, передача `detection_params` в конструктор `WorkingArea`.
    *   В `calib.py`: Улучшены сообщения `print`, небольшая реорганизация.
    *   В `robot_comm.py`: Добавлена проверка соединения перед отправкой данных. Улучшены сообщения.
    *   В `variables.py`: Код оставлен практически без изменений, так как он выполняет свою роль хранилища констант. Убрано дублирование `source_type`. Добавлена проверка и создание `APP_DIR`, если не существует.

Вот полностью обновленный код:

**`main_refactored.py`**

```python
# main_refactored.py
import sys
import os
import numpy as np
import cv2
import socket
import threading
import time
import logging
import json
from functools import partial
from concurrent.futures import ThreadPoolExecutor

from PyQt5.QtWidgets import (
    QApplication, QMainWindow, QWidget, QTabWidget, QHBoxLayout,
    QVBoxLayout, QPushButton, QLabel, QLineEdit, QFormLayout,
    QComboBox, QFileDialog, QMessageBox, QSizePolicy, QCheckBox, QSlider
)
from PyQt5.QtCore import Qt, QEventLoop
from PyQt5.QtGui import QImage, QPixmap

# Локальные импорты
import variables as var
from robot_comm import RobotComm
from calib import calibrate_camera_gui
from ultimate import WorkingArea, ObjectDetector, undistort_frame
from debug_window import DebugWindow  # Предполагаем, что DebugWindow вынесен в отдельный файл
from utils import Utils  # Вспомогательный класс

logging.basicConfig(filename=var.LOG_FILE,
                    level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

class MainWindow(QMainWindow):
    DEFAULT_DETECTION_PARAMS = {
        "SCALE": var.SCALE,
        "BLUR_KERNEL": var.BLUR_KERNEL,
        "CANNY_LOW": var.CANNY_LOW,
        "CANNY_HIGH": var.CANNY_HIGH,
        "MIN_AREA": var.MIN_AREA,
        "MAX_AREA": var.MAX_AREA,
        "CONVERSION_FACTOR": var.CONVERSION_FACTOR,
    }

    def __init__(self):
        super().__init__()
        self._init_state()
        self._load_config()
        self._init_ui()
        self._connect_signals()
        self.working_area_processor = WorkingArea(
            detection_params=self.detection_params,
            confirmation_callback=self.confirm_working_area,
            parent=self
        )

    def _init_state(self):
        """Инициализирует переменные состояния окна."""
        self.robot = None
        self.working_area_mask = None
        self.last_overlay_frame = None  # Для показа кадра с областью
        self._detector = None
        self._working_thread = None
        self._calib_thread = None
        self._working_stop_flag = False
        self._calib_stop_flag = False
        self._is_working = False
        self._is_calibrating = False
        self.param_widgets = {} # Словарик для виджетов параметров

    def _load_config(self):
        """Загружает конфигурацию (параметры детекции)."""
        self.detection_params = self.DEFAULT_DETECTION_PARAMS.copy()
        try:
            if os.path.exists(var.PARAMETERS_CONFIG):
                with open(var.PARAMETERS_CONFIG, "r") as f:
                    loaded_params = json.load(f)
                    # Обновляем только существующие ключи, не добавляем новые из файла
                    for key in self.detection_params:
                        if key in loaded_params:
                            self.detection_params[key] = loaded_params[key]
                    print("[INFO] Detection parameters loaded from:", var.PARAMETERS_CONFIG)
            else:
                print("[INFO] No saved parameter config found. Using defaults.")
                # Сохраняем дефолтные параметры, если файла нет
                self.save_detection_params()

        except Exception as e:
            print(f"[ERROR] Failed to load or process detection parameters: {e}")
            QMessageBox.warning(self, "Config Error",
                                f"Could not load detection parameters from {var.PARAMETERS_CONFIG}.\nUsing default values.\nError: {e}")
            # Перезаписываем дефолтными значениями в случае ошибки чтения
            self.detection_params = self.DEFAULT_DETECTION_PARAMS.copy()


    def _init_ui(self):
        """Создает и настраивает пользовательский интерфейс."""
        self.setWindowTitle("Pill Project Pilot GUI")
        self.resize(1000, 700) # Немного увеличим размер

        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        main_layout = QVBoxLayout(central_widget)

        # Верхний горизонтальный макет для вкладок и панели параметров
        top_layout = QHBoxLayout()

        # --- Вкладки ---
        self.tabs = QTabWidget()
        self.tabs.addTab(self._create_working_mode_tab(), "Working Mode")
        # Заглушки для нереализованных вкладок
        self.tabs.addTab(QLabel("Data Collection (Not Implemented)"), "Data Collection")
        self.tabs.addTab(QLabel("Data Analysis (Not Implemented)"), "Data Analysis")
        self.tabs.addTab(self._create_modbus_tab(), "Modbus TCP")
        self.tabs.addTab(self._create_calibration_tab(), "Calibration")
        top_layout.addWidget(self.tabs, 1) # Даем вкладкам больше места

        # --- Панель управления параметрами ---
        param_control_widget = QWidget()
        param_control_widget.setLayout(self._create_param_controls())
        param_control_widget.setFixedWidth(200) # Фиксируем ширину панели
        top_layout.addWidget(param_control_widget)

        main_layout.addLayout(top_layout)

        # Обновляем UI на основе текущего состояния (например, начальные источники)
        self.update_calibration_source_ui()
        self.update_working_source_ui()

    def _connect_signals(self):
        """Подключает сигналы виджетов к слотам (методам)."""
        # Вкладка Modbus
        self.modbus_refresh_button.clicked.connect(self._scan_network_for_modbus)
        self.modbus_device_selector.currentTextChanged.connect(lambda ip: self.modbus_ip_input.setText(ip))
        self.modbus_connect_button.clicked.connect(self._connect_modbus)
        self.modbus_disconnect_button.clicked.connect(self._disconnect_modbus)

        # Вкладка Калибровка
        self.calib_source_selector.currentIndexChanged.connect(self.update_calibration_source_ui)
        self.calib_video_browse_button.clicked.connect(self._browse_calibration_video)
        self.calib_video_path_input.mousePressEvent = lambda e: self._browse_calibration_video() # Клик на поле
        self.calib_camera_index_input.editingFinished.connect(self._update_calibration_camera_index)
        self.calib_start_button.clicked.connect(self.start_calibration)
        self.calib_stop_button.clicked.connect(self.stop_calibration)

        # Вкладка Рабочий режим
        self.working_source_selector.currentIndexChanged.connect(self.update_working_source_ui)
        self.working_video_browse_button.clicked.connect(self._browse_working_video)
        self.working_video_path_input.mousePressEvent = lambda e: self._browse_working_video() # Клик на поле
        self.working_camera_index_input.editingFinished.connect(self._update_working_camera_index)
        self.working_start_button.clicked.connect(self.start_working_mode)
        self.working_stop_button.clicked.connect(self.stop_working_mode)

        # Панель параметров
        self.debug_checkbox.stateChanged.connect(self._toggle_debug_window)


    # --- Вспомогательные методы создания UI ---

    def _create_source_selector_row(self, label_text, items, current_index=0):
        """Создает строку с Label и ComboBox для выбора источника."""
        layout = QHBoxLayout()
        layout.addWidget(QLabel(label_text))
        combo_box = QComboBox()
        combo_box.addItems(items)
        combo_box.setCurrentIndex(current_index)
        layout.addWidget(combo_box)
        return layout, combo_box

    def _create_file_path_row(self, label_text, default_path):
        """Создает строку для выбора пути к видеофайлу."""
        layout = QHBoxLayout()
        layout.addWidget(QLabel(label_text))
        line_edit = QLineEdit(default_path)
        line_edit.setReadOnly(True) # Делаем ReadOnly, выбор только через кнопку/клик
        browse_button = QPushButton("...")
        browse_button.setFixedWidth(30)
        layout.addWidget(line_edit)
        layout.addWidget(browse_button)
        return layout, line_edit, browse_button

    def _create_index_input_row(self, label_text, default_index):
        """Создает строку для ввода индекса камеры."""
        layout = QHBoxLayout()
        layout.addWidget(QLabel(label_text))
        line_edit = QLineEdit(str(default_index))
        layout.addWidget(line_edit)
        return layout, line_edit

    # --- Создание вкладок ---

    def _create_modbus_tab(self):
        tab = QWidget()
        layout = QVBoxLayout(tab)
        layout.setAlignment(Qt.AlignTop)

        # Сканирование и выбор устройства
        scan_layout = QHBoxLayout()
        self.modbus_device_selector = QComboBox()
        self.modbus_refresh_button = QPushButton("Scan Network")
        scan_layout.addWidget(QLabel("Found Devices:"))
        scan_layout.addWidget(self.modbus_device_selector, 1)
        scan_layout.addWidget(self.modbus_refresh_button)
        layout.addLayout(scan_layout)

        # Ввод IP и порта
        form_layout = QFormLayout()
        self.modbus_ip_input = QLineEdit(var.MODBUS_TCP_HOST)
        self.modbus_port_input = QLineEdit(str(var.MODBUS_TCP_PORT))
        form_layout.addRow("IP Address:", self.modbus_ip_input)
        form_layout.addRow("Port:", self.modbus_port_input)
        layout.addLayout(form_layout)

        # Кнопки управления и статус
        button_layout = QHBoxLayout()
        self.modbus_connect_button = QPushButton("Connect")
        self.modbus_disconnect_button = QPushButton("Disconnect")
        button_layout.addWidget(self.modbus_connect_button)
        button_layout.addWidget(self.modbus_disconnect_button)
        layout.addLayout(button_layout)

        self.modbus_status_label = QLabel("Status: Disconnected")
        layout.addWidget(self.modbus_status_label)

        # Начальное состояние кнопок
        self.modbus_disconnect_button.setEnabled(False)

        return tab

    def _create_calibration_tab(self):
        tab = QWidget()
        layout = QVBoxLayout(tab)
        layout.setAlignment(Qt.AlignTop)

        # Выбор источника
        source_layout, self.calib_source_selector = self._create_source_selector_row(
            "Calibration Source:", ["Calibration video", "Camera"]
        )
        layout.addLayout(source_layout)

        # Параметры для видео
        self.calib_video_layout_container = QWidget() # Контейнер для скрытия/показа
        video_container_layout = QVBoxLayout(self.calib_video_layout_container)
        video_container_layout.setContentsMargins(0,0,0,0)
        video_row_layout, self.calib_video_path_input, self.calib_video_browse_button = self._create_file_path_row(
            "Video Path:", var.CALIBRATION_VIDEO_PATH
        )
        video_container_layout.addLayout(video_row_layout)
        layout.addWidget(self.calib_video_layout_container)

        # Параметры для камеры
        self.calib_camera_layout_container = QWidget() # Контейнер для скрытия/показа
        camera_container_layout = QVBoxLayout(self.calib_camera_layout_container)
        camera_container_layout.setContentsMargins(0,0,0,0)
        camera_row_layout, self.calib_camera_index_input = self._create_index_input_row(
             "Camera Index:", var.CAMERA_INDEX
        )
        camera_container_layout.addLayout(camera_row_layout)
        layout.addWidget(self.calib_camera_layout_container)

        # Кнопки управления калибровкой
        calib_button_row = QHBoxLayout()
        self.calib_start_button = QPushButton("Start Calibration")
        self.calib_stop_button = QPushButton("Stop Calibration")
        self.calib_stop_button.setEnabled(False) # Стоп неактивен изначально
        calib_button_row.addWidget(self.calib_start_button)
        calib_button_row.addWidget(self.calib_stop_button)
        layout.addLayout(calib_button_row)

        # Область предпросмотра видео
        self.calib_video_label = QLabel("Calibration preview will appear here")
        self.calib_video_label.setMinimumSize(320, 240)
        self.calib_video_label.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        self.calib_video_label.setAlignment(Qt.AlignCenter)
        self.calib_video_label.setStyleSheet("background-color: black; color: grey;")
        layout.addWidget(self.calib_video_label, 1) # Занимает оставшееся место

        return tab

    def _create_working_mode_tab(self):
        tab = QWidget()
        layout = QVBoxLayout(tab)
        layout.setAlignment(Qt.AlignTop)

        # Выбор источника
        source_layout, self.working_source_selector = self._create_source_selector_row(
            "Working Source:", ["Working video", "Camera"]
        )
        layout.addLayout(source_layout)

        # Параметры для видео
        self.working_video_layout_container = QWidget() # Контейнер
        video_container_layout = QVBoxLayout(self.working_video_layout_container)
        video_container_layout.setContentsMargins(0,0,0,0)
        video_row, self.working_video_path_input, self.working_video_browse_button = self._create_file_path_row(
            "Video Path:", var.WORKING_VIDEO_PATH
        )
        video_container_layout.addLayout(video_row)
        layout.addWidget(self.working_video_layout_container)

        # Параметры для камеры
        self.working_camera_layout_container = QWidget() # Контейнер
        camera_container_layout = QVBoxLayout(self.working_camera_layout_container)
        camera_container_layout.setContentsMargins(0,0,0,0)
        camera_row, self.working_camera_index_input = self._create_index_input_row(
            "Camera Index:", var.CAMERA_INDEX
        )
        camera_container_layout.addLayout(camera_row)
        layout.addWidget(self.working_camera_layout_container)

        # Кнопки управления и опция Modbus
        button_row = QHBoxLayout()
        self.working_start_button = QPushButton("Start Working Mode")
        self.working_stop_button = QPushButton("Stop Working Mode")
        self.modbus_checkbox = QCheckBox("Send data via Modbus")
        self.working_stop_button.setEnabled(False)
        button_row.addWidget(self.working_start_button)
        button_row.addWidget(self.working_stop_button)
        button_row.addWidget(self.modbus_checkbox)
        layout.addLayout(button_row)

        # Лейбл для подтверждения рабочей зоны (изначально скрыт)
        self.working_area_confirm_label = QLabel("Is this the correct working area?")
        self.working_area_confirm_label.setAlignment(Qt.AlignCenter)
        self.working_area_confirm_label.setVisible(False)
        layout.addWidget(self.working_area_confirm_label)

        # Кнопки подтверждения рабочей зоны (изначально скрыты)
        self.working_confirm_layout_widget = QWidget() # Widget to hold confirm buttons layout
        self.working_confirm_layout = QHBoxLayout(self.working_confirm_layout_widget)
        self.confirm_button_yes = QPushButton("Yes, Confirm Area")
        self.confirm_button_no = QPushButton("No, Try Again")
        self.working_confirm_layout.addWidget(self.confirm_button_yes)
        self.working_confirm_layout.addWidget(self.confirm_button_no)
        self.working_confirm_layout_widget.setVisible(False)
        layout.addWidget(self.working_confirm_layout_widget)


        # Область отображения видео
        self.working_video_label = QLabel("Working mode video will appear here")
        self.working_video_label.setMinimumSize(320, 240)
        self.working_video_label.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)
        self.working_video_label.setAlignment(Qt.AlignCenter)
        self.working_video_label.setStyleSheet("background-color: black; color: grey;")
        layout.addWidget(self.working_video_label, 1) # Занимает оставшееся место


        return tab

    def _create_param_controls(self):
        panel_layout = QVBoxLayout()
        panel_layout.setAlignment(Qt.AlignTop)
        panel_layout.setSpacing(10)

        title_label = QLabel("Detection Parameters")
        title_label.setAlignment(Qt.AlignCenter)
        panel_layout.addWidget(title_label)

        self.debug_checkbox = QCheckBox("Show Debug View")
        panel_layout.addWidget(self.debug_checkbox)
        panel_layout.addSpacing(10)

        param_specs = { # Определения параметров: имя -> (min, max, step, тип(int/float))
            "SCALE": (0.1, 1.5, 0.01, float),
            "BLUR_KERNEL": (1, 21, 2, int), # Шаг 2 для нечетных
            "CANNY_LOW": (0, 255, 1, int),
            "CANNY_HIGH": (0, 255, 1, int),
            "MIN_AREA": (0, 10000, 50, int),
            "MAX_AREA": (100, 100000, 100, int),
            "CONVERSION_FACTOR": (0.01, 10.0, 0.01, float) # Добавили управление фактором
        }

        self.param_widgets = {}

        for name, (min_val, max_val, step, val_type) in param_specs.items():
            value = self.detection_params.get(name, self.DEFAULT_DETECTION_PARAMS.get(name)) # Берем из текущих или дефолт

            container = QVBoxLayout()
            container.setSpacing(2)

            # 1. Label
            param_label = QLabel(f"{name}:") # Добавим двоеточие
            param_label.setAlignment(Qt.AlignLeft)
            container.addWidget(param_label)

            # 2. Horizontal row: Slider + Value Label
            hbox = QHBoxLayout()
            slider = QSlider(Qt.Horizontal)

            # Масштабирование для слайдера
            if val_type == int:
                slider_min = min_val
                slider_max = max_val
                slider_value = int(value)
                slider.setSingleStep(step if name != "BLUR_KERNEL" else 1) # Обычный шаг для инт
                slider.setTickInterval(step)
            else: # float
                # Для float используем множитель, чтобы слайдер работал с int
                multiplier = 1 / step
                slider_min = int(min_val * multiplier)
                slider_max = int(max_val * multiplier)
                slider_value = int(value * multiplier)
                slider.setSingleStep(1) # Шаг слайдера всегда 1 для float

            slider.setMinimum(slider_min)
            slider.setMaximum(slider_max)
            slider.setValue(slider_value)

            value_label = QLabel(f"{value:.2f}" if val_type==float else str(value))
            value_label.setFixedWidth(45) # Чуть шире
            value_label.setAlignment(Qt.AlignRight)

            hbox.addWidget(slider, 1) # Слайдер растягивается
            hbox.addWidget(value_label)
            container.addLayout(hbox)


            # Функция обновления значения (универсальная)
            def update_value(param_name, param_spec, current_slider_val, lbl_widget, slider_widget):
                p_min, p_max, p_step, p_type = param_spec

                if p_type == float:
                    multiplier = 1 / p_step
                    new_val = current_slider_val / multiplier
                else: # int
                    new_val = current_slider_val
                    # Коррекция для BLUR_KERNEL (делаем нечетным)
                    if param_name == "BLUR_KERNEL" and new_val % 2 == 0:
                       # Если четное, берем ближайшее нечетное > 0
                       new_val = max(1, new_val - 1)
                       # Обновляем сам слайдер, если значение скорректировалось
                       slider_widget.setValue(new_val)
                       # Возвращаемся, чтобы не обновить параметр неправильным четным значением
                       return

                # Ограничение по min/max
                new_val = max(p_min, min(p_max, new_val))

                # Форматируем значение
                formatted_val = f"{new_val:.2f}" if p_type == float else str(new_val)
                display_val = new_val if p_type == int else round(new_val, 2)

                # Обновляем глобальный словарь параметров
                if self.detection_params.get(param_name) != display_val:
                    self.detection_params[param_name] = display_val
                    #print(f"[PARAM] {param_name} set to {display_val}")
                    lbl_widget.setText(formatted_val)
                    # Опционально: сохранять параметры при каждом изменении?
                    # self.save_detection_params()


            # Привязываем сигнал слайдера к функции обновления
            # Используем partial для передачи неизменяемых аргументов
            slider.valueChanged.connect(partial(update_value, name, param_specs[name], lbl_widget=value_label, slider_widget=slider))

            # Сохраняем виджеты для возможного доступа
            self.param_widgets[name] = (param_label, value_label, slider)
            panel_layout.addLayout(container)

        panel_layout.addStretch(1) # Добавляем растягивающееся пространство вниз

        # Кнопка сохранения параметров вручную
        save_button = QPushButton("Save Params")
        save_button.clicked.connect(self.save_detection_params)
        panel_layout.addWidget(save_button)

        return panel_layout

    # --- Методы обновления UI ---

    def update_calibration_source_ui(self):
        """Обновляет видимость элементов управления в зависимости от источника калибровки."""
        is_video = self.calib_source_selector.currentText() == "Calibration video"
        self.calib_video_layout_container.setVisible(is_video)
        self.calib_camera_layout_container.setVisible(not is_video)

    def update_working_source_ui(self):
        """Обновляет видимость элементов управления в зависимости от рабочего источника."""
        is_video = self.working_source_selector.currentText() == "Working video"
        self.working_video_layout_container.setVisible(is_video)
        self.working_camera_layout_container.setVisible(not is_video)


    # --- Обработчики событий и логика ---

    # Modbus
    def _connect_modbus(self):
        ip = self.modbus_ip_input.text()
        port_text = self.modbus_port_input.text()
        try:
            port = int(port_text)
            if not (0 < port < 65536):
                raise ValueError("Port out of range")
        except ValueError:
            QMessageBox.warning(self, "Input Error", "Invalid Port number. Please enter a number between 1 and 65535.")
            return

        # Закрываем старое соединение, если оно было
        if self.robot and self.robot.connected:
            self.robot.disconnect()

        self.robot = RobotComm(host=ip, port=port)
        self.robot.connect()

        if self.robot.connected:
            self.modbus_status_label.setText(f"Status: ✅ Connected to {ip}:{port}")
            self.modbus_connect_button.setEnabled(False)
            self.modbus_disconnect_button.setEnabled(True)
            logging.info(f"Successfully connected to Modbus server at {ip}:{port}")
        else:
            self.modbus_status_label.setText("Status: ❌ Connection failed")
            QMessageBox.critical(self, "Connection Error", f"Failed to connect to Modbus server at {ip}:{port}.")
            self.modbus_connect_button.setEnabled(True)
            self.modbus_disconnect_button.setEnabled(False)
            logging.error(f"Failed to connect to Modbus server at {ip}:{port}")
            self.robot = None # Убираем неудачный экземпляр

    def _disconnect_modbus(self):
        if self.robot and self.robot.connected:
            self.robot.disconnect()
        self.modbus_status_label.setText("Status: 🔌 Disconnected")
        self.modbus_connect_button.setEnabled(True)
        self.modbus_disconnect_button.setEnabled(False)
        self.robot = None
        logging.info("Disconnected from Modbus server by user.")

    def _scan_network_for_modbus(self):
        subnet = ".".join(self.modbus_ip_input.text().split('.')[:3]) + "." # Берем первые 3 октета из текущего IP
        port = int(self.modbus_port_input.text())
        self.modbus_device_selector.clear()
        self.modbus_status_label.setText("Status: Scanning...")
        QApplication.processEvents() # Обновляем UI перед долгим сканированием

        found_ips = []

        def check_ip(ip):
            try:
                with socket.create_connection((ip, port), timeout=0.2): # Уменьшим таймаут
                    return ip
            except (socket.timeout, OSError): # Ловим конкретные ошибки
                return None

        # Используем ThreadPoolExecutor для параллельного сканирования
        with ThreadPoolExecutor(max_workers=50) as executor:
            futures = [executor.submit(check_ip, f"{subnet}{i}") for i in range(1, 255)]
            for future in futures:
                result = future.result()
                if result:
                    found_ips.append(result)
                    self.modbus_device_selector.addItem(result) # Добавляем по мере нахождения
                    QApplication.processEvents() # Позволяем UI обновляться

        if found_ips:
            self.modbus_status_label.setText(f"Status: Scan complete. Found {len(found_ips)} device(s)")
            self.modbus_device_selector.setCurrentIndex(0) # Выбираем первый найденный
            self.modbus_ip_input.setText(self.modbus_device_selector.currentText()) # Обновляем поле ввода IP
        else:
            self.modbus_status_label.setText("Status: Scan complete. No devices found")
            QMessageBox.information(self, "Scan Results", f"No Modbus devices found on subnet {subnet}* at port {port}.")


    # Calibration
    def _browse_calibration_video(self):
        file_path, _ = QFileDialog.getOpenFileName(self, "Select Calibration Video", var.APP_DIR, "Video Files (*.mp4 *.avi *.mov)")
        if file_path:
            self.calib_video_path_input.setText(file_path)
            var.CALIBRATION_VIDEO_PATH = file_path # Обновляем глобальную переменную (спорно, но сохраняем логику)
            print(f"Selected calibration video: {file_path}")

    def _update_calibration_camera_index(self):
         try:
            index = int(self.calib_camera_index_input.text())
            if index < 0: raise ValueError("Index must be non-negative")
            var.CAMERA_INDEX = index # Обновляем глобальную
            print(f"Updated calibration CAMERA_INDEX to: {index}")
         except ValueError as e:
            QMessageBox.warning(self, "Input Error", f"Invalid Camera Index: {e}. Please enter a non-negative integer.")
            self.calib_camera_index_input.setText(str(var.CAMERA_INDEX)) # Возвращаем старое значение


    def start_calibration(self):
        if self._is_calibrating:
            print("[WARN] Calibration is already running.")
            return

        self._is_calibrating = True
        self._calib_stop_flag = False
        self.calib_start_button.setEnabled(False)
        self.calib_stop_button.setEnabled(True)
        self.calib_video_label.setText("Starting calibration...")
        QApplication.processEvents()


        source = self.calib_source_selector.currentText()
        if source == "Calibration video":
            source_type = "video"
            video_path = self.calib_video_path_input.text()
            camera_index = -1 # Не используется
            if not os.path.exists(video_path):
                QMessageBox.critical(self, "File Error", f"Calibration video file not found:\n{video_path}")
                self._cleanup_calibration()
                return
        else: # Camera
            source_type = "camera"
            video_path = None
            try:
                camera_index = int(self.calib_camera_index_input.text())
            except ValueError:
                 QMessageBox.critical(self, "Input Error", "Invalid Camera Index for calibration.")
                 self._cleanup_calibration()
                 return

        # Callbacks для обновления GUI и завершения
        def frame_callback_gui(frame):
            # Этот код выполняется в потоке калибровки, обновляем QLabel
            try:
                pixmap = Utils.frame_to_qpixmap(frame, self.calib_video_label)
                self.calib_video_label.setPixmap(pixmap)
            except Exception as e:
                print(f"[ERROR] Error updating calibration preview: {e}") # Не фатально

        def done_callback_gui(camera_matrix, dist_coeffs):
            # Этот код выполняется в потоке калибровки по завершению
            if camera_matrix is not None:
                QMessageBox.information(self, "Calibration Result", "Calibration successful!\nResults saved to " + var.CALIBRATION_FILE)
                print("[INFO] Calibration successful.")
            else:
                 # Проверяем, была ли остановка запрошена пользователем
                if self._calib_stop_flag:
                    QMessageBox.warning(self, "Calibration Result", "Calibration stopped by user.")
                    print("[INFO] Calibration stopped by user.")
                else:
                    QMessageBox.critical(self, "Calibration Result", "Calibration failed or could not gather enough data.")
                    print("[ERROR] Calibration failed.")

            # Важно: обновлять UI из основного потока
            self.parent()._schedule_task(self._cleanup_calibration)


        print(f"[INFO] Starting calibration with source: {source_type} (Index/Path: {camera_index if source_type=='camera' else video_path})")
        # Запуск калибровки в фоновом потоке
        self._calib_thread = threading.Thread(
            target=calibrate_camera_gui,
            args=(source_type, camera_index, video_path, var.MAX_CAPTURES),
            kwargs={
                'frame_callback': frame_callback_gui,
                'done_callback': done_callback_gui,
                'stop_flag_getter': lambda: self._calib_stop_flag
            },
            daemon=True
        )
        self._calib_thread.start()

    def stop_calibration(self):
        if not self._is_calibrating:
            return
        print("[INFO] Requesting calibration stop...")
        self._calib_stop_flag = True
        # Поток сам завершится и вызовет done_callback -> _cleanup_calibration


    def _cleanup_calibration(self):
        """Сбрасывает состояние после завершения/остановки калибровки."""
        print("[INFO] Cleaning up calibration state...")
        self._is_calibrating = False
        self._calib_stop_flag = False # Сбрасываем флаг
        self.calib_start_button.setEnabled(True)
        self.calib_stop_button.setEnabled(False)
        self.calib_video_label.setText("Calibration preview will appear here")
        self.calib_video_label.setStyleSheet("background-color: black; color: grey;") # Возвращаем стиль
        self.calib_thread = None # Очищаем ссылку на поток
        QApplication.processEvents() # Обновляем интерфейс


    # Working Mode
    def _browse_working_video(self):
        file_path, _ = QFileDialog.getOpenFileName(self, "Select Working Video", var.APP_DIR, "Video Files (*.mp4 *.avi *.mov)")
        if file_path:
            self.working_video_path_input.setText(file_path)
            var.WORKING_VIDEO_PATH = file_path # Обновляем глобальную
            print(f"Selected working video: {file_path}")

    def _update_working_camera_index(self):
         try:
            index = int(self.working_camera_index_input.text())
            if index < 0: raise ValueError("Index must be non-negative")
            # Можно обновить var.CAMERA_INDEX здесь, если хотим синхронизировать с калибровкой
            print(f"Updated working CAMERA_INDEX to: {index}")
         except ValueError as e:
            QMessageBox.warning(self, "Input Error", f"Invalid Camera Index: {e}. Please enter a non-negative integer.")
            self.working_camera_index_input.setText(str(var.CAMERA_INDEX)) # Возвращаем значение по умолчанию


    def start_working_mode(self):
        if self._is_working:
            print("[WARN] Working mode is already running.")
            return

        # --- 1. Проверки и подготовка ---
        self.working_start_button.setEnabled(False) # Блокируем кнопку Start сразу

        # Проверка калибровочного файла
        if not os.path.exists(var.CALIBRATION_FILE):
             QMessageBox.critical(self, "Setup Error", f"Calibration file not found: {var.CALIBRATION_FILE}\nPlease run calibration first.")
             self.working_start_button.setEnabled(True)
             return
        try:
            calib_data = np.load(var.CALIBRATION_FILE)
            cameraMatrix = calib_data['cameraMatrix']
            distCoeffs = calib_data['distCoeffs']
        except Exception as e:
             QMessageBox.critical(self, "Setup Error", f"Error loading calibration data from {var.CALIBRATION_FILE}:\n{e}")
             self.working_start_button.setEnabled(True)
             return

        # Проверка файла идентификации (некритично, если его нет)
        identification_config_path = var.IDENTIFICATION_CONFIG
        if not os.path.exists(identification_config_path):
            print(f"[WARN] Identification config {identification_config_path} not found. Recognition might be limited.")
            QMessageBox.warning(self, "Config Warning", f"Identification config file not found:\n{identification_config_path}\nObject recognition might be limited to basic features.")
            # identification_config_path = None # Можно и так, но ObjectDetector справится с путем

        # Открытие источника видео
        source = self.working_source_selector.currentText()
        if source == "Working video":
            video_path = self.working_video_path_input.text()
            if not os.path.exists(video_path):
                 QMessageBox.critical(self, "File Error", f"Working video file not found:\n{video_path}")
                 self.working_start_button.setEnabled(True)
                 return
            cap = cv2.VideoCapture(video_path)
        else: # Camera
            try:
                index = int(self.working_camera_index_input.text())
                cap = cv2.VideoCapture(index)
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, var.CAMERA_WIDTH)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, var.CAMERA_HEIGHT)
                cap.set(cv2.CAP_PROP_FPS, var.CAMERA_FPS)
            except ValueError:
                QMessageBox.critical(self, "Input Error", "Invalid Camera Index for working mode.")
                self.working_start_button.setEnabled(True)
                return
            except Exception as e:
                 QMessageBox.critical(self, "Camera Error", f"Failed to open camera {index}: {e}")
                 self.working_start_button.setEnabled(True)
                 return

        if not cap.isOpened():
             QMessageBox.critical(self, "Source Error", f"Failed to open video source ({source}).")
             self.working_start_button.setEnabled(True)
             return

        # --- 2. Определение рабочей области (синхронно в основном потоке) ---
        self.working_video_label.setText("Detecting working area...")
        QApplication.processEvents()
        working_area_confirmed = False
        max_attempts = 5 # Попробуем найти область на нескольких кадрах
        attempt = 0
        while attempt < max_attempts and not working_area_confirmed:
            ret, frame = cap.read()
            if not ret:
                 QMessageBox.critical(self, "Source Error", "Failed to read frame from video source to detect working area.")
                 cap.release()
                 self.working_start_button.setEnabled(True)
                 return

            print(f"[INFO] Attempt {attempt+1} to detect working area...")
            # Важно: передаем актуальные параметры детекции
            self.working_area_processor.detection_params = self.detection_params
            confirmed = self._prepare_working_area(frame) # Эта функция покажет диалог подтверждения

            if confirmed is None: # Ошибка детекции
                print("[INFO] Working area not found in this frame.")
                attempt += 1
                continue
            elif confirmed is True: # Пользователь подтвердил
                working_area_confirmed = True
                print("[INFO] Working area confirmed by user.")
                break
            else: # Пользователь отказался
                print("[INFO] User rejected the working area. Trying next frame...")
                attempt += 1 # Пытаемся снова на следующем кадре

        if not working_area_confirmed:
            QMessageBox.warning(self, "Setup Failed", "Could not confirm the working area after several attempts. Please check lighting and camera view.")
            cap.release()
            self._cleanup_working_mode() # Используем единый метод очистки
            return

        # Обновляем фактор конверсии, если он был найден
        if "CONVERSION_FACTOR" in self.working_area_processor.detection_params:
             self.detection_params["CONVERSION_FACTOR"] = self.working_area_processor.detection_params["CONVERSION_FACTOR"]
             print(f"[PARAM] Updated CONVERSION_FACTOR from working area: {self.detection_params['CONVERSION_FACTOR']:.4f}")
             # Обновляем UI слайдера (если есть)
             self._update_param_widget("CONVERSION_FACTOR")

        # --- 3. Инициализация детектора и запуск потока ---
        self._is_working = True
        self._working_stop_flag = False
        self._working_frame_counter = 0
        self._working_records = []

        # Создаем детектор с загруженной конфигурацией и актуальными параметрами
        self._detector = ObjectDetector(
            identification_config_path=identification_config_path,
            detection_params=self.detection_params
        )

        self.working_video_label.setText("Working mode started...")
        self.working_stop_button.setEnabled(True) # Активируем кнопку Стоп
        QApplication.processEvents()

        # Запускаем рабочий поток
        self._working_thread = threading.Thread(
            target=self._working_mode_thread_loop,
            args=(cap, cameraMatrix, distCoeffs), # Передаем VideoCapture и калибровку
            daemon=True
        )
        self._working_thread.start()


    def stop_working_mode(self):
        if not self._is_working:
            return
        print("[INFO] Requesting working mode stop...")
        self._working_stop_flag = True
        self.working_start_button.setEnabled(False) # Пока поток не завершится, Start недоступен
        self.working_stop_button.setEnabled(False) # Stop тоже блокируем на время остановки
        # Поток сам завершится и вызовет _cleanup_working_mode

    def _working_mode_thread_loop(self, cap, cameraMatrix, distCoeffs):
        """Основной цикл обработки кадров в рабочем режиме (выполняется в потоке)."""
        print("[THREAD] Working mode thread started.")
        frame_errors = 0
        max_frame_errors = 10

        while not self._working_stop_flag:
            # Обновляем параметры детектора перед обработкой кадра (на случай изменения в GUI)
            # Создаем копию, чтобы избежать проблем с гонкой данных при обновлении из GUI
            current_detection_params = self.detection_params.copy()
            if self._detector:
                 self._detector.detection_params = current_detection_params

            ret, frame = cap.read()
            if not ret:
                frame_errors += 1
                print(f"[THREAD][WARN] Failed to read frame ({frame_errors}/{max_frame_errors})")
                if frame_errors >= max_frame_errors or not cap.get(cv2.CAP_PROP_POS_FRAMES): # Если видео закончилось
                   print("[THREAD] Video ended or reached max frame read errors.")
                   break
                time.sleep(0.1) # Пауза при ошибке чтения
                continue
            else:
                 frame_errors = 0 # Сброс счетчика ошибок при удачном чтении


            # 1. Коррекция кадра
            corrected_frame = cv2.undistort(frame, cameraMatrix, distCoeffs)

            # 2. Обработка детектором
            if self._detector and self.working_area_mask is not None:
                # Process frame передает копии detection_params внутрь, так что не страшно
                result = self._detector.process_frame(corrected_frame,
                                                      self._working_frame_counter,
                                                      mask=self.working_area_mask,
                                                      mode="work") # Укажем режим
                if result is None:
                    # print("[THREAD] No objects detected or error in processing.")
                    # Отображаем просто скорректированный кадр без детекций
                    display_image = corrected_frame
                    detections = [] # Пустой список детекций
                    edges = np.zeros_like(corrected_frame[:,:,0]) # Пустое изображение граней
                else:
                    edges, objects_overlayed, detections = result
                    display_image = objects_overlayed # Кадр с наложенными объектами

                # Обновление окна отладки (если открыто)
                if hasattr(self, 'debug_window') and self.debug_window.isVisible():
                    contours_img = np.zeros_like(display_image) # Создаем черное изображение того же размера
                    raw_contours = [d['contour'] for d in detections] # Получаем контуры из детекций
                    # Отрисовываем контуры, используя текущий фактор конверсии для масштаба координат пикселей
                    # Важно: `drawContours` работает с пиксельными координатами контуров, не `_mm`
                    conv_factor = current_detection_params.get("CONVERSION_FACTOR", 1.0)
                    if conv_factor == 0: conv_factor = 1.0 # Предохранитель от деления на 0

                    pixel_contours = []
                    for det_cont in raw_contours:
                        # Предполагается, что 'contour' в detection хранит пиксельные координаты.
                        # Если это не так, нужно конвертировать _mm обратно в пиксели
                         pixel_contours.append(det_cont) # Используем как есть

                    if pixel_contours:
                        cv2.drawContours(contours_img, pixel_contours, -1, (0, 255, 0), 1) # Зеленые контуры

                    # Подготовка Canny (если нужно для отладки)
                    # Можно пересчитать Canny с текущими параметрами для DebugWindow
                    gray = cv2.cvtColor(corrected_frame, cv2.COLOR_BGR2GRAY)
                    blur_k = current_detection_params.get("BLUR_KERNEL", 5)
                    blur_k = max(1, blur_k if blur_k % 2 != 0 else blur_k - 1) # Гарантируем нечетность > 0
                    blur = cv2.GaussianBlur(gray, (blur_k, blur_k), 0)
                    canny_l = current_detection_params.get("CANNY_LOW", 50)
                    canny_h = current_detection_params.get("CANNY_HIGH", 150)
                    debug_canny_edges = cv2.Canny(blur, canny_l, canny_h)
                    # Конвертируем Canny в BGR для DebugWindow
                    debug_canny_edges_bgr = cv2.cvtColor(debug_canny_edges, cv2.COLOR_GRAY2BGR)


                    # Используем _schedule_task для безопасного вызова метода DebugWindow
                    # schedule_task(self.debug_window.update_images, debug_canny_edges_bgr, contours_img)

                    # Прямой вызов (может быть небезопасен, если DebugWindow делает сложные вещи)
                    try:
                        # Делаем копии перед передачей, чтобы избежать проблем с потоками
                        self.debug_window.update_images(debug_canny_edges_bgr.copy(), contours_img.copy())
                    except Exception as e:
                         print(f"[THREAD][WARN] Error updating debug window: {e}")


            else:
                 print("[THREAD][WARN] Detector or working area mask not initialized.")
                 display_image = corrected_frame # Показываем просто исправленный кадр
                 detections = []


            # 3. Обновление GUI (отображение кадра)
            # pixmap = Utils.frame_to_qpixmap(display_image, self.working_video_label)
            # Schedule task to update pixmap in the main thread
            # self.parent()._schedule_task(self.working_video_label.setPixmap, pixmap)

            # Прямой вызов setPixmap из потока (менее безопасный, но часто работает в PyQt)
            try:
                pixmap = Utils.frame_to_qpixmap(display_image, self.working_video_label)
                self.working_video_label.setPixmap(pixmap)
            except Exception as e:
                print(f"[THREAD][WARN] Error setting pixmap in working mode: {e}")


            # 4. Отправка данных (если включено и есть соединение)
            # Используем schedule_task для вызова send_robot_data из основного потока? Или проверяем здесь?
            # Проще проверить и отправить прямо из этого потока, RobotComm должен быть потокобезопасным (библиотека pyModbusTCP обычно да)

            send_via_modbus = self.modbus_checkbox.isChecked() # Читаем состояние чекбокса (относительно безопасно)

            if send_via_modbus and self.robot and self.robot.is_connected():
                 if detections:
                     self.send_robot_data(detections) # Используем метод главного окна
                 # else:
                     # print("[THREAD] No detections to send via Modbus.")
            elif send_via_modbus:
                if not self.robot or not self.robot.is_connected():
                    # Логируем ошибку только один раз или периодически
                    # if self._working_frame_counter % 60 == 0: # Раз в ~2 секунды при 30fps
                    #     print("[THREAD][WARN] Modbus sending enabled, but robot is not connected.")
                    pass # Не спамим консоль


            # Выводим информацию в консоль, если Modbus не включен
            if not send_via_modbus and detections:
                 # print(f"--- Frame {self._working_frame_counter} Detections ---")
                 for d in detections:
                    # category = d.get('predicted_category', 'unknown')
                    # center = d.get('center', (0,0))
                    # size = (d.get('width', 0), d.get('height', 0))
                    # track_id = d.get('track_id', -1)
                    # print(f"  ID {track_id} | Cat: {category} | Center(mm): ({center[0]:.1f}, {center[1]:.1f}) | Size(mm): {size[0]:.1f}x{size[1]:.1f}")
                    pass # Пока отключим вывод в консоль


            self._working_frame_counter += 1
            # Небольшая пауза, чтобы не перегружать CPU и дать GUI отреагировать
            time.sleep(0.01)

        # --- Цикл завершен (либо stop_flag, либо ошибка) ---
        print("[THREAD] Working mode thread finished.")
        cap.release()
        print("[THREAD] Video capture released.")

        # Экспорт данных (если нужно)
        # self._export_detection_data()

        # Запланировать очистку состояния в основном потоке
        self.parent()._schedule_task(self._cleanup_working_mode)

    def _cleanup_working_mode(self):
        """Сбрасывает состояние после завершения/остановки рабочего режима."""
        print("[INFO] Cleaning up working mode state...")
        self._is_working = False
        self._working_stop_flag = False
        self.working_start_button.setEnabled(True)
        self.working_stop_button.setEnabled(False)
        self.working_video_label.setText("Working mode video will appear here")
        self.working_video_label.setStyleSheet("background-color: black; color: grey;")
        # Скрываем элементы подтверждения рабочей области
        self.working_area_confirm_label.setVisible(False)
        self.working_confirm_layout_widget.setVisible(False)

        self.working_area_mask = None
        self.last_overlay_frame = None
        self._detector = None
        self._working_thread = None # Очищаем ссылку на поток
        print("[INFO] Working mode cleanup complete.")
        QApplication.processEvents()

    def send_robot_data(self, detections):
         """Отправляет данные обнаруженных объектов роботу."""
         if not self.robot or not self.robot.is_connected():
             # print("[WARN] Attempted to send data, but robot is not connected.")
             return

         # Примерное маппирование категорий на коды (можно вынести в variables.py)
         category_mapping = {
            "circular_white": 1, "circle_white": 1,
            "circular_pink": 2, "circle_red": 2, # Допустим, розовый это красный
            "circular_black": 3, "circle_black": 3,
            "rhombus_white": 4, "rectangle_white": 4, # Допустим, ромб это прямоугольник
            "rhombus_pink": 5, "rectangle_red": 5,
            "rhombus_black": 6, "rectangle_black": 6,
            # Добавить другие категории...
            "unknown": 0,
        }

         for det in detections:
            # Получаем данные из словаря детекции
            x_mm = det.get('center', (0,0))[0]
            y_mm = det.get('center', (0,0))[1]
            width_mm = det.get('width', 0)
            height_mm = det.get('height', 0)
            angle = det.get('angle', 0)
            category_name = det.get('predicted_category', "unknown").lower() # Приводим к нижнему регистру для надежности
            category_code = category_mapping.get(category_name, 0) # 0 для неизвестных/ненайденных
            obj_id = det.get('track_id', 0) # Используем ID трекера

            # Отправка данных через RobotComm
            # print(f"[SEND] ID:{obj_id}, Cat:{category_code}({category_name}), X:{x_mm:.1f}, Y:{y_mm:.1f}, W:{width_mm:.1f}, H:{height_mm:.1f}, Ang:{angle:.1f}")
            success = self.robot.send_data(
                obj_id=obj_id,
                x_mm=x_mm,
                y_mm=y_mm,
                width_mm=width_mm,
                height_mm=height_mm,
                angle=angle,
                category_code=category_code
            )
            if not success:
                 print(f"[ERROR] Failed to send data for object ID {obj_id} via Modbus.")
                 # Возможно, стоит разорвать соединение или предпринять другие действия при ошибке

    def confirm_working_area(self, overlay_frame):
        """
        Показывает предложенную рабочую область и запрашивает подтверждение у пользователя.
        Выполняется СИНХРОННО в основном потоке GUI. Блокирует вызвавший поток (должен быть основной).
        Возвращает True (подтверждено), False (отклонено) или None (ошибка/неожиданное закрытие).
        """
        print("[GUI] Showing working area confirmation dialog...")
        self._confirmation_result = None  # Результат подтверждения

        # --- Обновляем UI для подтверждения ---
        self.working_area_confirm_label.setVisible(True)
        self.working_confirm_layout_widget.setVisible(True)

        # Показываем предложенный кадр
        pixmap = Utils.frame_to_qpixmap(overlay_frame, self.working_video_label)
        self.working_video_label.setPixmap(pixmap)

        # Создаем и используем QEventLoop для ожидания ответа
        loop = QEventLoop()

        # Локальные функции для обработки нажатий кнопок
        def accept_area():
            print("[GUI] User confirmed the working area.")
            self._confirmation_result = True
            # Восстанавливаем UI
            self.working_area_confirm_label.setVisible(False)
            self.working_confirm_layout_widget.setVisible(False)
            loop.quit()

        def reject_area():
            print("[GUI] User rejected the working area.")
            self._confirmation_result = False
            # Восстанавливаем UI
            self.working_area_confirm_label.setVisible(False)
            self.working_confirm_layout_widget.setVisible(False)
            loop.quit()

        # Отсоединяем старые сигналы (если были) и подключаем новые
        try:
            self.confirm_button_yes.clicked.disconnect()
            self.confirm_button_no.clicked.disconnect()
        except TypeError: # Сигналы не были подключены
             pass
        self.confirm_button_yes.clicked.connect(accept_area)
        self.confirm_button_no.clicked.connect(reject_area)

        # Запускаем цикл событий, ждем loop.quit()
        loop.exec_()

        print(f"[GUI] Working area confirmation result: {self._confirmation_result}")
        return self._confirmation_result

    def _prepare_working_area(self, frame):
        """
        Пытается обнаружить рабочую область на кадре и запрашивает подтверждение.
        Возвращает True (подтверждено), False (отклонено пользователем), None (не найдено).
        Вызывается из основного потока ПЕРЕД запуском рабочего потока.
        """
        print("[SETUP] Detecting working area...")
        result = self.working_area_processor.objectDetection(frame)

        if result is None:
            print("[SETUP] Working area not detected in this frame.")
            # Не показываем пользователю ничего, просто возвращаем None
            # Можно показать исходный кадр, если нужно
            # pixmap = Utils.frame_to_qpixmap(frame, self.working_video_label)
            # self.working_video_label.setPixmap(pixmap)
            # QApplication.processEvents()
            return None # Не найдено
        else:
            overlay_frame, working_mask, conversion_factor = result
            print(f"[SETUP] Candidate working area found. Conversion factor ~ {conversion_factor:.4f}. Asking for confirmation.")

            # Вызываем confirm_working_area, который покажет overlay_frame и диалог
            user_confirmed = self.confirm_working_area(overlay_frame)

            if user_confirmed is True:
                 # Сохраняем маску и кадр для возможного показа
                 self.working_area_mask = working_mask
                 self.last_overlay_frame = overlay_frame
                 # Важно: Обновляем CONVERSION_FACTOR в *основном* словаре параметров
                 self.detection_params["CONVERSION_FACTOR"] = conversion_factor
                 print("[SETUP] Working area confirmed and stored.")
                 return True # Подтверждено
            elif user_confirmed is False:
                print("[SETUP] User rejected the area.")
                # Сбрасываем маску
                self.working_area_mask = None
                self.last_overlay_frame = None
                return False # Отклонено
            else: # user_confirmed is None (ошибка в диалоге?)
                 print("[SETUP][WARN] Confirmation dialog returned an unexpected value.")
                 return None # Считаем ошибкой / не найденным


    # Parameter Panel related
    def _toggle_debug_window(self, state):
        if state == Qt.Checked:
            if not hasattr(self, 'debug_window') or not self.debug_window:
                self.debug_window = DebugWindow(parent=self) # Передаем parent
            self.debug_window.show()
        else:
            if hasattr(self, 'debug_window') and self.debug_window:
                self.debug_window.hide() # Просто скрываем, чтобы не создавать заново

    def _update_param_widget(self, param_name):
        """Обновляет виджет параметра (слайдер и лейбл) на основе текущего значения в self.detection_params."""
        if param_name in self.param_widgets and param_name in self.detection_params:
            param_label, value_label, slider = self.param_widgets[param_name]
            value = self.detection_params[param_name]

            # Находим спецификацию параметра (немного неэффективно, но для редких обновлений пойдет)
            param_spec = None
            # Должен быть определен param_specs в _create_param_controls
            # Мы можем сохранить его как член класса self.param_specs
            if hasattr(self, 'param_specs') and param_name in self.param_specs:
                 param_spec = self.param_specs[param_name]
            else: # Если specs не сохранены, попытаемся угадать тип
                 if isinstance(value, float): p_type = float; p_step=0.01
                 else: p_type=int; p_step=1
                 if param_name == "BLUR_KERNEL": p_step = 2
                 # Значения min/max и step тут не известны, только тип
                 param_spec = (0, 100, p_step, p_type) # Dummy spec

            p_min, p_max, p_step, p_type = param_spec

            # Обновление Label
            formatted_val = f"{value:.2f}" if p_type == float else str(value)
            value_label.setText(formatted_val)

            # Обновление Slider
            try:
                if p_type == float:
                    multiplier = 1.0 / p_step
                    slider_value = int(value * multiplier)
                else: # int
                    slider_value = int(value)
                # Отключаем сигналы слайдера на время установки значения, чтобы избежать рекурсии/петли
                slider.blockSignals(True)
                slider.setValue(slider_value)
                slider.blockSignals(False)
            except Exception as e:
                print(f"[ERROR] Failed to update slider for {param_name}: {e}")

    def save_detection_params(self):
        """Сохраняет текущие параметры детекции в JSON файл."""
        try:
            # Сохраняем только параметры, которые есть в DEFAULT_DETECTION_PARAMS
            params_to_save = {k: self.detection_params.get(k, self.DEFAULT_DETECTION_PARAMS[k])
                              for k in self.DEFAULT_DETECTION_PARAMS}

            with open(var.PARAMETERS_CONFIG, "w") as f:
                json.dump(params_to_save, f, indent=4) # Используем отступ 4 для читаемости
            print(f"[SAVE] Detection parameters saved to {var.PARAMETERS_CONFIG}")
            logging.info(f"Detection parameters saved to {var.PARAMETERS_CONFIG}")
        except Exception as e:
            error_msg = f"Failed to save detection parameters to {var.PARAMETERS_CONFIG}:\n{e}"
            print(f"[ERROR] {error_msg}")
            QMessageBox.critical(self, "Save Error", error_msg)
            logging.error(error_msg)


    def closeEvent(self, event):
        """Обработчик события закрытия окна."""
        print("Shutting down application...")
        logging.info("Application shutdown sequence started.")

        # 1. Останавливаем активные процессы
        if self._is_working:
            print("Stopping working mode...")
            self._working_stop_flag = True
            if self._working_thread and self._working_thread.is_alive():
                print("Waiting for working thread to finish...")
                self._working_thread.join(timeout=2.0) # Даем потоку время завершиться
                if self._working_thread.is_alive():
                    print("[WARN] Working thread did not finish cleanly.")
                    logging.warning("Working thread did not finish cleanly on shutdown.")
            print("Working mode stopped.")

        if self._is_calibrating:
            print("Stopping calibration...")
            self._calib_stop_flag = True
            if self._calib_thread and self._calib_thread.is_alive():
                print("Waiting for calibration thread to finish...")
                self._calib_thread.join(timeout=2.0)
                if self._calib_thread.is_alive():
                    print("[WARN] Calibration thread did not finish cleanly.")
                    logging.warning("Calibration thread did not finish cleanly on shutdown.")
            print("Calibration stopped.")

        # 2. Отключаемся от робота
        if self.robot and self.robot.is_connected():
            print("Disconnecting from Modbus robot...")
            self.robot.disconnect()
            print("Disconnected.")

        # 3. Сохраняем параметры
        print("Saving detection parameters...")
        self.save_detection_params()

        # 4. Закрываем окно отладки (если есть и видимо)
        if hasattr(self, 'debug_window') and self.debug_window:
             print("Closing debug window...")
             self.debug_window.close()


        print("Shutdown complete. Exiting.")
        logging.info("Application shutdown sequence complete.")
        event.accept()

    # Вспомогательная функция для безопасного выполнения задач в основном потоке GUI
    # Нужна, если используется прямой вызов GUI методов из других потоков
    # Вместо schedule_task можно использовать Qt Signals/Slots для более надежного подхода

    # Пример реализации schedule_task (если нужен)
    # from PyQt5.QtCore import QTimer
    # def _schedule_task(self, func, *args, **kwargs):
    #      """ Выполняет функцию func в основном потоке GUI """
    #      QTimer.singleShot(0, lambda: func(*args, **kwargs))



if __name__ == '__main__':
    # Создаем директорию для данных, если её нет
    if not os.path.exists(var.APP_DIR):
        try:
            os.makedirs(var.APP_DIR)
            print(f"Created application data directory: {var.APP_DIR}")
        except OSError as e:
            print(f"Error creating directory {var.APP_DIR}: {e}")
            sys.exit(1) # Выход, если не можем создать директорию

    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec_())
```

**`debug_window.py`** (Вынесенный класс `DebugWindow`)

```python
# debug_window.py
import cv2
import numpy as np
from PyQt5.QtWidgets import QWidget, QHBoxLayout, QLabel
from PyQt5.QtCore import Qt
from PyQt5.QtGui import QImage, QPixmap

from utils import Utils # Используем общую утилиту

class DebugWindow(QWidget):
    """Окно для отображения отладочной информации (Canny, контуры)."""
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Detection Debug View")
        self.setMinimumSize(640, 320)
        self.resize(800, 400) # Установим размер по умолчанию

        layout = QHBoxLayout(self)

        self.canny_label = QLabel("Canny Edges")
        self.canny_label.setAlignment(Qt.AlignCenter)
        self.canny_label.setStyleSheet("background-color: #333; color: white;")
        self.canny_label.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)

        self.contour_label = QLabel("Detected Contours")
        self.contour_label.setAlignment(Qt.AlignCenter)
        self.contour_label.setStyleSheet("background-color: #333; color: white;")
        self.contour_label.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)

        layout.addWidget(self.canny_label)
        layout.addWidget(self.contour_label)

        self.canny_image = None
        self.contour_image = None

    def update_images(self, canny_img, contour_img):
        """Обновляет отображаемые изображения."""
        # Делаем копии на всякий случай, если оригиналы меняются в другом потоке
        self.canny_image = canny_img.copy() if canny_img is not None else None
        self.contour_image = contour_img.copy() if contour_img is not None else None
        self._update_display() # Обновляем отображение

    def _update_display(self):
        """Обновляет QPixmap в QLabel на основе текущих изображений."""
        if not self.isVisible(): # Не обновляем, если окно скрыто
            return

        if self.canny_image is not None:
            try:
                pixmap_canny = Utils.frame_to_qpixmap(self.canny_image, self.canny_label)
                self.canny_label.setPixmap(pixmap_canny)
            except Exception as e:
                print(f"[DebugWindow][ERROR] Failed to update Canny image: {e}")
                self.canny_label.setText("Error Canny")
        else:
            self.canny_label.setText("No Canny Image")

        if self.contour_image is not None:
            try:
                pixmap_contour = Utils.frame_to_qpixmap(self.contour_image, self.contour_label)
                self.contour_label.setPixmap(pixmap_contour)
            except Exception as e:
                print(f"[DebugWindow][ERROR] Failed to update Contour image: {e}")
                self.contour_label.setText("Error Contours")
        else:
            self.contour_label.setText("No Contour Image")

    def resizeEvent(self, event):
        """Перерисовывает изображения при изменении размера окна."""
        self._update_display()
        super().resizeEvent(event)

    def closeEvent(self, event):
        """Срабатывает при закрытии окна (например, через 'X')."""
        # Просто скрываем окно, а не удаляем, если оно управляется из MainWindow
        self.hide()
        event.ignore() # Игнорируем событие, чтобы MainWindow мог управлять им

        # Если окно должно быть полностью уничтожено при закрытии,
        # нужно сообщить MainWindow (например, через сигнал),
        # чтобы он сбросил self.debug_window = None
```

**`utils.py`** (Новый файл для утилит)

```python
# utils.py
import cv2
import numpy as np
from PyQt5.QtGui import QImage, QPixmap
from PyQt5.QtWidgets import QWidget, QLabel, QHBoxLayout, QVBoxLayout, QLayout

class Utils:
    """Вспомогательный класс с общими статическими методами."""

    @staticmethod
    def frame_to_qpixmap(frame: np.ndarray, target_widget: QWidget) -> QPixmap:
        """
        Конвертирует кадр OpenCV (BGR) в QPixmap, масштабируя его
        для вписывания в target_widget с сохранением пропорций.

        :param frame: Кадр OpenCV (numpy array, BGR).
        :param target_widget: Виджет PyQt (например, QLabel), в который вписывается изображение.
        :return: QPixmap для отображения.
        """
        if frame is None or frame.size == 0:
            return QPixmap() # Возвращаем пустой Pixmap, если кадр невалидный

        try:
            frame_h, frame_w = frame.shape[:2]
            label_w = target_widget.width()
            label_h = target_widget.height()

            if frame_h == 0 or frame_w == 0 or label_w <= 0 or label_h <= 0:
                 return QPixmap() # Защита от деления на ноль или некорректных размеров

            # Масштабирование с сохранением пропорций
            scale = min(label_w / frame_w, label_h / frame_h)
            new_w = int(frame_w * scale)
            new_h = int(frame_h * scale)

            # Проверка на случай нулевых размеров после масштабирования
            if new_w <= 0 or new_h <= 0:
                 return QPixmap()

            resized_frame = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)

            # Создание QImage
            if len(resized_frame.shape) == 3: # Цветное изображение (BGR)
                rgb_image = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)
                h, w, ch = rgb_image.shape
                bytes_per_line = ch * w
                q_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
            elif len(resized_frame.shape) == 2: # Серое изображение
                h, w = resized_frame.shape
                bytes_per_line = w
                q_image = QImage(resized_frame.data, w, h, bytes_per_line, QImage.Format_Grayscale8)
            else:
                return QPixmap() # Неподдерживаемый формат

            # Создание черного холста для центрирования
            canvas = QImage(label_w, label_h, QImage.Format_RGB888)
            canvas.fill(Qt.black)

            # Рассчитываем смещение для центрирования
            x_offset = (label_w - new_w) // 2
            y_offset = (label_h - new_h) // 2

            # Рисуем отмасштабированное изображение на холсте (требует QPainter)
            from PyQt5.QtGui import QPainter
            painter = QPainter(canvas)
            painter.drawImage(x_offset, y_offset, q_image)
            painter.end()


            return QPixmap.fromImage(canvas)

        except Exception as e:
            print(f"[ERROR] Failed to convert frame to QPixmap: {e}")
            return QPixmap() # Возвращаем пустой Pixmap в случае ошибки

    @staticmethod
    def toggle_layout_visibility(layout: QLayout, visible: bool):
            """Рекурсивно изменяет видимость всех виджетов внутри макета."""
            if layout is None:
                return
            for i in range(layout.count()):
                item = layout.itemAt(i)
                widget = item.widget()
                sub_layout = item.layout()

                if widget is not None:
                    widget.setVisible(visible)
                elif sub_layout is not None:
                    Utils.toggle_layout_visibility(sub_layout, visible)

```

**`ultimate.py` (Небольшие правки)**

```python
# ultimate.py (с небольшими правками)
import cv2
import numpy as np
# import random # Не используется
import math
import pandas as pd
import json
import os
from scipy.optimize import linear_sum_assignment
# from PyQt5.QtWidgets import QMessageBox # QMessageBox лучше показывать из GUI потока (main.py)
import variables as var # Используем псевдоним var

# Функция вынесена для переиспользования
def undistort_frame(frame, calibration_file=var.CALIBRATION_FILE):
    """Корректирует дисторсию кадра с использованием данных калибровки."""
    if not os.path.exists(calibration_file):
        print(f"[WARN] Calibration file not found: {calibration_file}. Returning original frame.")
        return frame

    try:
        data = np.load(calibration_file)
        cameraMatrix = data['cameraMatrix']
        distCoeffs = data['distCoeffs']
        # print("Calibration data loaded for undistortion.") # Уменьшим количество логов
    except Exception as e:
        print(f"[ERROR] Error loading calibration data from {calibration_file}: {e}")
        return frame # Возвращаем оригинальный кадр при ошибке

    if cameraMatrix is not None and distCoeffs is not None:
        try:
            return cv2.undistort(frame, cameraMatrix, distCoeffs)
        except Exception as e:
             print(f"[ERROR] Error during cv2.undistort: {e}")
             return frame
    else:
         print("[WARN] cameraMatrix or distCoeffs are None in calibration file.")
         return frame


class WorkingArea:
    """Класс для обнаружения и подтверждения рабочей области."""
    def __init__(self,
                 detection_params, # Ожидаем словарь с параметрами
                 calibration_file=var.CALIBRATION_FILE,
                 confirmation_callback=None, # Функция обратного вызова для подтверждения (в GUI)
                 parent=None): # Ссылка на родительский виджет (если нужна)

        # Сохраняем переданные параметры как начальные
        self.detection_params = detection_params.copy() if detection_params else {}
        self.calibration_file = calibration_file
        self.confirmation_callback = confirmation_callback
        self.parent = parent
        print("[WorkingArea] Initialized.")

    def objectDetection(self, frame):
        """
        Обнаруживает потенциальную рабочую область на кадре.

        :param frame: Входной кадр (BGR).
        :return: Кортеж (overlay_frame, working_mask, conversion_factor) при успехе,
                 None, если область не найдена или не соответствует критериям.
        """
        print("[WorkingArea] Detecting candidate working area...")
        if frame is None:
            print("[WorkingArea][ERROR] Input frame is None.")
            return None

        try:
            corrected_frame = undistort_frame(frame, self.calibration_file)
            if corrected_frame is None: # Обработка ошибки из undistort_frame
                 corrected_frame = frame # Пробуем работать с оригиналом

            # Параметры из словаря self.detection_params
            blur_kernel = self.detection_params.get("BLUR_KERNEL", 5)
             # Гарантируем нечетность и >= 1
            blur_kernel = max(1, blur_kernel if blur_kernel % 2 != 0 else blur_kernel - 1)
            canny_low = self.detection_params.get("CANNY_LOW", 50)
            canny_high = self.detection_params.get("CANNY_HIGH", 150)

            gray = cv2.cvtColor(corrected_frame, cv2.COLOR_BGR2GRAY)
            blur = cv2.GaussianBlur(gray, (blur_kernel, blur_kernel), 0)
            edges = cv2.Canny(blur, canny_low, canny_high)
            # print("[WorkingArea] Preprocessing (undistort, blur, canny) done.")

            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            print(f"[WorkingArea] Found {len(contours)} raw contours.")

            frame_height, frame_width = corrected_frame.shape[:2]
            min_frame_dim = min(frame_width, frame_height)
            max_frame_dim = max(frame_width, frame_height)
            min_ratio = var.WORKING_AREA_MIN_SIZE_RATIO
            max_ratio = var.WORKING_AREA_MAX_SIZE_RATIO

            candidate_found = False
            for cnt in contours:
                # Быстрая проверка площади перед дорогой minAreaRect
                area = cv2.contourArea(cnt)
                # Пропускаем очень маленькие контуры сразу
                # (Используем процент от площади кадра как эвристику)
                if area < (min_frame_dim * max_frame_dim * 0.01): # Например, меньше 1% площади кадра
                     continue

                try:
                    rect = cv2.minAreaRect(cnt)
                except Exception as e:
                    print(f"[WorkingArea][WARN] cv2.minAreaRect failed for a contour: {e}")
                    continue # Пропускаем этот контур

                (cx, cy), (width, height), angle = rect

                # Защита от нулевых размеров
                if width <= 0 or height <= 0:
                    continue

                short_side, long_side = sorted([width, height])

                # Относительные размеры
                short_ratio = short_side / min_frame_dim
                long_ratio = long_side / max_frame_dim
                # print(f"[WorkingArea DEBUG] Candidate: Area={area:.0f}, Short={short_side:.1f} ({short_ratio:.2f}), Long={long_side:.1f} ({long_ratio:.2f}), Angle={angle:.1f}")


                # Проверка соотношений сторон к размерам кадра
                if (min_ratio <= short_ratio <= max_ratio and
                    min_ratio <= long_ratio <= max_ratio):
                    print(f"[WorkingArea] ✅ Found candidate passing ratio check (Area: {area:.0f}px)")
                    candidate_found = True

                    box = cv2.boxPoints(rect).astype(np.int32)
                    workArea_overlay = corrected_frame.copy()
                    cv2.drawContours(workArea_overlay, [box], 0, (0, 0, 255), 3) # Рисуем красным и потолще

                    # Вызываем callback для подтверждения пользователем
                    # Этот callback должен быть реализован в GUI и запускать confirm_working_area
                    user_confirmed = self.confirmation_callback(workArea_overlay) if self.confirmation_callback else True

                    if user_confirmed:
                        print("[WorkingArea] ✅ User confirmed the area.")
                        working_mask = np.zeros(corrected_frame.shape[:2], dtype="uint8")
                        cv2.drawContours(working_mask, [box], 0, 255, -1) # Залитая маска

                        # Рассчитываем фактор конверсии на основе ИЗВЕСТНОЙ ширины объекта (A4?)
                        # или на основе одного из размеров найденной области, если она эталонная
                        # В оригинальном коде было 210.0 / short_side - это похоже на A4 ширину
                        # Оставим эту логику, но сделаем REFERENCE_OBJECT_WIDTH_MM настраиваемым
                        conversion_factor = var.REFERENCE_OBJECT_WIDTH_MM / short_side
                        # Сохраняем фактор в параметрах самого экземпляра WorkingArea
                        # Главный класс потом скопирует его в свои detection_params
                        self.detection_params["CONVERSION_FACTOR"] = conversion_factor
                        print(f"[WorkingArea] Calculated Conversion Factor: {conversion_factor:.4f} (RefWidth={var.REFERENCE_OBJECT_WIDTH_MM} / FoundShortSide={short_side:.2f})")

                        return workArea_overlay, working_mask, conversion_factor
                    else:
                        print("[WorkingArea] User rejected candidate area. Continuing search...")
                        # Продолжаем искать следующий подходящий контур
                # else:
                     # print(f"[WorkingArea DEBUG] Candidate failed ratio check: short_r={short_ratio:.2f}, long_r={long_ratio:.2f}")


            if not candidate_found:
                 print("[WorkingArea] No suitable candidate working area found after checking all contours.")
            return None # Ничего подходящего не найдено

        except Exception as e:
            print(f"[WorkingArea][ERROR] Unexpected error during working area detection: {e}")
            import traceback
            traceback.print_exc()
            return None

class ObjectDetector:
    """Класс для детекции, распознавания и трекинга объектов."""
    def __init__(self, identification_config_path=None,
                 detection_params=None, # Ожидаем словарь с параметрами
                 confirmation_callback=None, # Не используется здесь напрямую
                 parent=None):

        # Используем переданные параметры или пустой словарь
        self.detection_params = detection_params.copy() if detection_params else {}
        # Устанавливаем дефолтные значения для отсутствующих ключей, если нужно
        # self.detection_params.setdefault("SOME_KEY", default_value)

        self.parent = parent # Ссылка на GUI (не используется здесь)
        self.records = [] # Список для записи данных в CSV
        self.next_track_id = 1
        self.tracks = [] # Активные треки объектов: {'id': int, 'last_center': tuple(mm), 'lost_frames': int, 'object_name': str}
        self.IDENTIFICATION_CONFIG = {} # { category_name: { parameters: { feature: {min, max}}, ... } }
        self.features_list = [] # Список имен фич, используемых для идентификации
        self.mask = None # Маска рабочей области (должна быть установлена извне)
        self.calibration_file = var.CALIBRATION_FILE # Путь к файлу калибровки
        self.MAX_TRACK_LOST_FRAMES = var.MAX_LOST_FRAMES # Сколько кадров трек может быть потерян
        self.MAX_DISTANCE_FOR_TRACKING = var.MAX_DISTANCE # Макс. расстояние для связи трека (в мм!)


        # Загрузка конфигурации распознавания
        self._load_identification_config(identification_config_path)
        print("[ObjectDetector] Initialized.")


    def _load_identification_config(self, config_path):
        """Загружает параметры распознавания объектов из JSON."""
        self.IDENTIFICATION_CONFIG = {}
        self.features_list = []
        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, "r") as f:
                    data = json.load(f)
                    self.IDENTIFICATION_CONFIG = data.get("categories", {})
                    self.features_list = data.get("features", [])
                    # Маску из этого конфига больше не загружаем, она приходит в process_frame
                    print(f"[ObjectDetector] Loaded identification config from: {config_path}")
                    print(f"  - Categories loaded: {list(self.IDENTIFICATION_CONFIG.keys())}")
                    print(f"  - Features for recognition: {self.features_list}")
            except Exception as e:
                print(f"[ObjectDetector][ERROR] Failed to load identification config from {config_path}: {e}")
        else:
            print(f"[ObjectDetector][WARN] Identification config path not provided or file not found: {config_path}. Recognition might be basic.")


    def process_frame(self, corrected_frame, frame_counter, mask=None, mode="work"):
        """
        Основной метод обработки кадра: детекция, распознавание, трекинг.

        :param corrected_frame: Кадр после коррекции дисторсии (BGR).
        :param frame_counter: Номер текущего кадра.
        :param mask: Бинарная маска рабочей области (numpy uint8).
        :param mode: Режим работы ('work', 'test', 'collect'). Не используется пока.
        :return: Кортеж (edges_image, objects_overlayed_image, list_of_detections) или None при ошибке.
                 list_of_detections: список словарей, описывающих каждый найденный объект.
        """
        if corrected_frame is None:
            print("[ObjectDetector][ERROR] Input frame is None.")
            return None
        if mask is None:
            print("[ObjectDetector][WARN] No working area mask provided! Processing the whole frame.")
            # Создаем маску, покрывающую весь кадр
            mask_to_use = np.ones(corrected_frame.shape[:2], dtype=np.uint8) * 255
        else:
            mask_to_use = mask

        try:
            # --- 1. Предобработка ---
            # Параметры из словаря self.detection_params (актуальные на момент вызова)
            blur_kernel = self.detection_params.get("BLUR_KERNEL", 5)
            blur_kernel = max(1, blur_kernel if blur_kernel % 2 != 0 else blur_kernel - 1)
            canny_low = self.detection_params.get("CANNY_LOW", 50)
            canny_high = self.detection_params.get("CANNY_HIGH", 150)
            min_area = self.detection_params.get("MIN_AREA", 100)
            max_area = self.detection_params.get("MAX_AREA", 10000)
             # !!! Получаем КОЭФФИЦИЕНТ конверсии ПИКСЕЛЕЙ в ММ !!!
            conv = self.detection_params.get("CONVERSION_FACTOR", 1.0)
            if conv <= 0:
                 print(f"[ObjectDetector][WARN] Invalid CONVERSION_FACTOR ({conv}), using 1.0")
                 conv = 1.0

            gray = cv2.cvtColor(corrected_frame, cv2.COLOR_BGR2GRAY)
            # Применяем маску ДО поиска контуров
            gray_masked = cv2.bitwise_and(gray, gray, mask=mask_to_use)
            blur = cv2.GaussianBlur(gray_masked, (blur_kernel, blur_kernel), 0)
            edges = cv2.Canny(blur, canny_low, canny_high)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)


            # --- 2. Фильтрация контуров и извлечение данных ---
            # Фильтруем контуры по площади ДО вычисления всего остального
            potential_contours = []
            for cnt in contours:
                area_px = cv2.contourArea(cnt)
                if min_area <= area_px <= max_area:
                     potential_contours.append(cnt)
                # else:
                #     print(f"[DEBUG] Contour rejected by area: {area_px} (Min: {min_area}, Max: {max_area})")


            detections = [] # Список необработанных детекций этого кадра
            # processed_centers = set() # Для предотвращения дублирования близких объектов (простая проверка)

            if not potential_contours:
                # print("[ObjectDetector] No contours passed area filter.")
                # Возвращаем исходный кадр и пустые детекции, но не None
                overlay_frame = corrected_frame.copy()
                cv2.putText(overlay_frame, f"Frame: {frame_counter} (No detections)", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)
                return edges, overlay_frame, []

            print(f"[ObjectDetector] Frame {frame_counter}: {len(potential_contours)} contours passed area filter.")

            for contour in potential_contours:
                try:
                    rect = cv2.minAreaRect(contour) # ((center_x, center_y), (width, height), angle) in pixels
                except Exception as e:
                    print(f"[ObjectDetector][WARN] cv2.minAreaRect failed for a contour: {e}")
                    continue

                center_px = self._get_center_pixels(rect)
                width_px, height_px = rect[1]
                angle = rect[2] # Угол в градусах

                # Проверка на нулевые размеры
                if width_px <= 0 or height_px <= 0:
                     continue

                # --- Предотвращение обработки слишком близких контуров (опционально) ---
                # is_duplicate = False
                # for existing_center in processed_centers:
                #      dist = math.dist(center_px, existing_center)
                #      if dist < 10: # Если центры ближе 10 пикселей - считаем дубликатом
                #           is_duplicate = True
                #           break
                # if is_duplicate:
                #      # print(f"[DEBUG] Duplicate contour rejected near {center_px}")
                #      continue
                # processed_centers.add(center_px)

                # --- Вычисление признаков ---
                features_px = self.compute_shape_features(contour, rect)
                avg_color_bgr = self.compute_average_color(corrected_frame, contour, mask_to_use)

                # Норма Hu моментов для сравнения
                hu_norm = np.linalg.norm(np.array(features_px["hu_moments"]))
                # Среднее значение цвета (можно использовать один канал или все)
                avg_color_val = np.mean(avg_color_bgr)

                # --- Конвертация в ММ ---
                # Центр
                center_mm = (center_px[0] * conv, center_px[1] * conv)
                # Размеры
                width_mm = width_px * conv
                height_mm = height_px * conv
                # Площадь и периметр
                area_mm2 = features_px["area"] * (conv ** 2)
                perimeter_mm = features_px["perimeter"] * conv
                # Глубина дефектов
                avg_defect_depth_mm = features_px["avg_defect_depth"] * conv


                # --- Создание словаря признаков для распознавания ---
                # Используем только те фичи, которые есть в features_list из конфига
                features_for_recognition = {
                    "aspect_ratio": features_px["aspect_ratio"], # Добавим в compute_shape_features
                    "area": area_mm2, # Используем мм^2 для распознавания? Или пиксели? ЗАВИСИТ ОТ КОНФИГА
                    "perimeter": perimeter_mm, # Аналогично - мм или пиксели?
                    "extent": features_px["extent"],
                    "hu_moments_norm": hu_norm,
                    "circularity": features_px["circularity"],
                    "convexity_defects_count": features_px["convexity_defects_count"],
                    "avg_defect_depth": avg_defect_depth_mm, # мм или пиксели?
                    "avg_color_val": avg_color_val # Значение цвета (0-255)
                }

                # --- Распознавание ---
                predicted_category = self._recognize_object(features_for_recognition)
                if predicted_category is None:
                    predicted_category = "unknown"

                # --- Формирование словаря детекции ---
                # Сохраняем как пиксельные, так и мм значения для гибкости
                detection = {
                    'frame': frame_counter,
                    'center_px': center_px,
                    'width_px': width_px,
                    'height_px': height_px,
                    'center': center_mm, # Координаты в мм (основные для робота)
                    'width': width_mm,    # Ширина в мм
                    'height': height_mm,   # Высота в мм
                    'angle': angle,        # Угол (общий)
                    'area_px': features_px["area"],
                    'area': area_mm2,
                    'aspect_ratio': features_px["aspect_ratio"],
                    'perimeter_px': features_px["perimeter"],
                    'perimeter': perimeter_mm,
                    'extent': features_px["extent"],
                    'hu_moments': features_px["hu_moments"], # Пиксельные
                    'circularity': features_px["circularity"],
                    'convexity_defects_count': features_px["convexity_defects_count"],
                    'avg_defect_depth_px': features_px["avg_defect_depth"],
                    'avg_defect_depth': avg_defect_depth_mm,
                    'avg_color': avg_color_bgr, # BGR цвет
                    'predicted_category': predicted_category,
                    'contour': contour # Контур в пикселях (нужен для отрисовки и, возможно, трекинга)
                }
                detections.append(detection)


            # --- 3. Трекинг (присвоение ID) ---
            # Присваиваем track_id к детекциям текущего кадра
            # Используем координаты в ММ для расчета расстояний
            tracked_detections = self._assign_ids(detections)


            # --- 4. Запись данных в CSV и Отрисовка ---
            objects_overlayed = corrected_frame.copy() # Создаем копию для рисования
            self._log_detections_to_record(tracked_detections) # Сохраняем для CSV

            # Отрисовываем рабочую область для наглядности (полупрозрачным цветом?)
            # overlay = objects_overlayed.copy()
            # cv2.drawContours(overlay, [mask_to_use], -1, (255, 0, 0), -1) # Синяя залитая область
            # alpha = 0.2 # Прозрачность
            # objects_overlayed = cv2.addWeighted(overlay, alpha, objects_overlayed, 1 - alpha, 0)
            # TODO: Маска может быть не одним контуром! Отрисовка сложнее. Пока не рисуем маску.

            for det in tracked_detections:
                 # Используем ПИКСЕЛЬНЫЕ координаты для отрисовки
                 cx_px, cy_px = det['center_px']
                 # label = f"ID {det['track_id']} | {det['predicted_category']}" # Короткий лейбл
                 # Полный лейбл (может быть слишком длинным)
                 label = f"ID {det.get('track_id','N/A')} | {det.get('predicted_category','unk')} | {det.get('width', 0):.1f}x{det.get('height', 0):.1f} mm"

                 # Отрисовка контура
                 cv2.drawContours(objects_overlayed, [det['contour']], -1, (0, 255, 0), 2) # Зеленый контур
                 # Отрисовка центра
                 cv2.circle(objects_overlayed, det['center_px'], 4, (0, 0, 255), -1) # Красный центр
                 # Отрисовка текста
                 # Размещаем текст рядом с центром
                 text_pos = (cx_px + 10, cy_px + 5)
                 cv2.putText(objects_overlayed, label, text_pos,
                            cv2.FONT_HERSHEY_SIMPLEX, 0.45, # Уменьшим шрифт
                            (255, 255, 255), 2, cv2.LINE_AA) # Белый текст с обводкой
                 cv2.putText(objects_overlayed, label, text_pos,
                            cv2.FONT_HERSHEY_SIMPLEX, 0.45,
                            (0, 0, 0), 1, cv2.LINE_AA) # Черный внутри для читаемости


            # Добавляем счетчик кадров на изображение
            cv2.putText(objects_overlayed, f"Frame: {frame_counter}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)


            # Экспорт CSV (возможно, делать реже?)
            if frame_counter % 60 == 0: # Каждые ~2 секунды
                 self.export_csv(os.path.join(var.APP_DIR, "detected_objects_log.csv"))

            # Возвращаем результат
            return edges, objects_overlayed, tracked_detections

        except Exception as e:
             print(f"[ObjectDetector][ERROR] Unexpected error processing frame {frame_counter}: {e}")
             import traceback
             traceback.print_exc()
             # Возвращаем что-то, чтобы главный цикл не падал
             return None, corrected_frame.copy(), [] # Грани?, исходный кадр, пустой список


    def _assign_ids(self, current_detections):
        """
        Присваивает или обновляет ID треков для текущих детекций
        используя алгоритм Венгерской оптимальной привязки (Hungarian algorithm).
        Работает с координатами в МИЛЛИМЕТРАХ.
        """
        # Центры активных треков (в мм)
        track_centers_mm = np.array([track['last_center'] for track in self.tracks])
        # Центры текущих детекций (в мм)
        detection_centers_mm = np.array([det['center'] for det in current_detections])

        # --- Матрица стоимости (расстояний) ---
        cost_matrix = np.empty((len(self.tracks), len(current_detections)))
        if len(self.tracks) > 0 and len(current_detections) > 0:
            # Используем broadcasting для вычисления матрицы евклидовых расстояний
             cost_matrix = np.linalg.norm(track_centers_mm[:, np.newaxis, :] - detection_centers_mm[np.newaxis, :, :], axis=2)
        elif len(self.tracks) == 0:
             # Если нет активных треков, присваиваем новые ID всем детекциям
             for i, det in enumerate(current_detections):
                 det['track_id'] = self.next_track_id
                 self.tracks.append({
                    'id': self.next_track_id,
                    'last_center': det['center'], # мм
                    'lost_frames': 0,
                    'object_name': det['predicted_category'] if det['predicted_category'] != "unknown" else None
                 })
                 self.next_track_id += 1
             return current_detections
        else: # Есть треки, но нет детекций - пропускаем назначение
             pass

        assigned_detection_indices = set() # Индексы детекций, которым присвоен ID трека

        # --- Применение Венгерского алгоритма ---
        if cost_matrix.size > 0:
             try:
                 # linear_sum_assignment находит пары с минимальной суммарной стоимостью (расстоянием)
                 row_ind, col_ind = linear_sum_assignment(cost_matrix)

                 # --- Обработка найденных пар (трек <-> детекция) ---
                 for r, c in zip(row_ind, col_ind):
                     track_index = r
                     detection_index = c
                     distance = cost_matrix[track_index, detection_index]

                     # Проверяем, не превышает ли расстояние порог
                     if distance < self.MAX_DISTANCE_FOR_TRACKING:
                         # Расстояние приемлемо - связываем трек и детекцию
                         track = self.tracks[track_index]
                         detection = current_detections[detection_index]

                         # Обновляем трек
                         track['last_center'] = detection['center'] # Новый центр в мм
                         track['lost_frames'] = 0 # Сбрасываем счетчик потерянных кадров
                         detection['track_id'] = track['id'] # Присваиваем ID трека детекции

                         # Обновляем имя объекта в треке, если детекция распознана
                         if detection['predicted_category'] != "unknown":
                             track['object_name'] = detection['predicted_category']
                         elif track.get('object_name'):
                            # Если детекция неизвестна, но трек ее помнит - используем имя из трека
                            detection['predicted_category'] = track['object_name']


                         assigned_detection_indices.add(detection_index) # Помечаем детекцию как обработанную
                     # else: Этот трек не соответствует этой детекции (слишком далеко)

             except ValueError as e:
                  print(f"[ObjectDetector][ERROR] Error in linear_sum_assignment: {e}")
                  # Может возникнуть, если cost_matrix содержит NaN или inf

        # --- Создание новых треков для необработанных детекций ---
        for i, det in enumerate(current_detections):
            if i not in assigned_detection_indices:
                # Эта детекция не была связана с существующим треком
                det['track_id'] = self.next_track_id
                self.tracks.append({
                    'id': self.next_track_id,
                    'last_center': det['center'], # мм
                    'lost_frames': 0,
                    'object_name': det['predicted_category'] if det['predicted_category'] != "unknown" else None
                })
                self.next_track_id += 1

        # --- Обновление и удаление потерянных треков ---
        active_tracks = []
        for track in self.tracks:
             # Проверяем, был ли трек обновлен в этом кадре
             found_in_current_frame = False
             for det in current_detections:
                 if det.get('track_id') == track['id']:
                     found_in_current_frame = True
                     break

             if not found_in_current_frame:
                 # Трек не найден в текущем кадре - увеличиваем счетчик потерь
                 track['lost_frames'] += 1
             else:
                  track['lost_frames'] = 0 # На всякий случай сбросим, если найден

             # Сохраняем трек, если он не потерян слишком долго
             if track['lost_frames'] <= self.MAX_TRACK_LOST_FRAMES:
                 active_tracks.append(track)
             # else:
                  # print(f"[Tracking] Track {track['id']} lost (>{self.MAX_TRACK_LOST_FRAMES} frames).")


        self.tracks = active_tracks # Обновляем список активных треков


        # Финальный проход: если категория неизвестна, но трек ее помнит
        for det in current_detections:
            if det.get('predicted_category') == "unknown":
                 track_id = det.get('track_id')
                 if track_id:
                     for track in self.tracks:
                         if track['id'] == track_id and track.get('object_name'):
                              det['predicted_category'] = track['object_name']
                              break # Нашли соответствующий трек

        return current_detections # Возвращаем детекции с обновленными/присвоенными track_id


    def _log_detections_to_record(self, tracked_detections):
        """Добавляет данные из детекций в список self.records для последующего экспорта."""
        for det in tracked_detections:
            # Собираем нужные поля
             self.records.append({
                'frame': det['frame'],
                'track_id': det.get('track_id', -1), # Используем ID трекера
                'category': det.get('predicted_category', 'unknown'),
                'center_x_mm': det.get('center', (0,0))[0],
                'center_y_mm': det.get('center', (0,0))[1],
                'width_mm': det.get('width', 0),
                'height_mm': det.get('height', 0),
                'angle_deg': det.get('angle', 0),
                'area_mm2': det.get('area', 0),
                'aspect_ratio': det.get('aspect_ratio', 0),
                'perimeter_mm': det.get('perimeter', 0),
                'extent': det.get('extent', 0),
                # 'hu_norm': np.linalg.norm(np.array(det['hu_moments'])), # Пересчитываем? или храним?
                'circularity': det.get('circularity', 0),
                'defects_count': det.get('convexity_defects_count', 0),
                'defect_depth_mm': det.get('avg_defect_depth', 0),
                'avg_color_b': det.get('avg_color', (0,0,0))[0],
                'avg_color_g': det.get('avg_color', (0,0,0))[1],
                'avg_color_r': det.get('avg_color', (0,0,0))[2],
             })


    def export_csv(self, path):
        """Экспортирует собранные данные в CSV файл."""
        if not self.records:
            # print("[EXPORT] No records to export.")
            return
        try:
            df = pd.DataFrame(self.records)
            df.to_csv(path, index=False)
            print(f"[EXPORT] Saved {len(df)} records to {path}")
            # Очистить записи после экспорта?
            # self.records = []
        except Exception as e:
            print(f"[EXPORT][ERROR] Failed to export records to CSV {path}: {e}")


    def _get_center_pixels(self, rect):
        """Возвращает целочисленные координаты центра из MinAreaRect."""
        return (int(rect[0][0]), int(rect[0][1]))


    def _recognize_object(self, features_record):
        """
        Пытается распознать объект на основе его признаков и конфигурации.

        :param features_record: Словарь с вычисленными признаками объекта.
        :return: Имя распознанной категории (str) или None.
        """
        if not self.IDENTIFICATION_CONFIG or not self.features_list:
             # print("[Recognize] No identification config loaded.")
             return None # Не можем распознать без конфига

        # Проходим по всем известным категориям в конфиге
        for category_name, config_data in self.IDENTIFICATION_CONFIG.items():
             category_params = config_data.get("parameters", {})
             match = True # Флаг соответствия текущей категории

             # Проверяем соответствие по всем фичам, указанным в features_list
             for feature_name in self.features_list:
                if feature_name not in features_record:
                    # print(f"[Recognize Debug] Feature '{feature_name}' not found in record for category '{category_name}'.")
                    match = False; break # Фича отсутствует в данных - не подходит

                if feature_name not in category_params:
                    # print(f"[Recognize Debug] Feature '{feature_name}' not defined in config for category '{category_name}'.")
                    match = False; break # Фича не описана в конфиге для этой категории - не подходит

                # Получаем границы из конфига и значение из данных
                limits = category_params[feature_name]
                min_val = limits.get("min", -np.inf) # По умолчанию - бесконечность
                max_val = limits.get("max", np.inf)  # По умолчанию + бесконечность
                current_val = features_record[feature_name]

                # Проверяем попадание в диапазон
                if not (min_val <= current_val <= max_val):
                     # print(f"[Recognize Debug] Feature '{feature_name}' mismatch for '{category_name}'. Value: {current_val:.2f}, Range: [{min_val:.2f}, {max_val:.2f}]")
                     match = False; break # Значение фичи вне диапазона - не подходит

             # Если все проверки пройдены (match остался True)
             if match:
                # print(f"[Recognize] Object recognized as: {category_name}")
                return category_name # Возвращаем имя найденной категории

        # Если ни одна категория не подошла
        # print("[Recognize] Object category is unknown.")
        return None


    def compute_shape_features(self, contour, rect):
        """Вычисляет геометрические признаки контура."""
        features = {}
        area = cv2.contourArea(contour)
        features["area"] = area

        perimeter = cv2.arcLength(contour, True)
        features["perimeter"] = perimeter

        # Bounding box (не повернутый)
        x, y, w, h = cv2.boundingRect(contour)

        # Extent = Area / BoundingBoxArea
        bbox_area = w * h
        features["extent"] = area / bbox_area if bbox_area > 0 else 0

        # Hu Moments
        try:
            moments = cv2.moments(contour)
            hu_moments = cv2.HuMoments(moments).flatten()
            # Логарифмическое масштабирование для уменьшения разброса
            features["hu_moments"] = [-np.sign(hu) * np.log10(abs(hu)) if hu != 0 else 0 for hu in hu_moments]
        except Exception: # Может быть деление на 0 или другие проблемы
            features["hu_moments"] = [0.0] * 7


        # Circularity = 4 * pi * Area / Perimeter^2
        features["circularity"] = (4 * math.pi * area / (perimeter ** 2)) if perimeter > 0 else 0

        # Convexity Defects
        defects_count = 0
        avg_defect_depth = 0.0
        try:
             if len(contour) > 3: # Нужно хотя бы 4 точки для дефектов
                hull_indices = cv2.convexHull(contour, returnPoints=False)
                if hull_indices is not None and len(hull_indices) > 2: # Выпуклая оболочка существует
                    # Важно: hull_indices может быть не отсортирован! Нужно передавать отсортированный?
                    # Смотрим документацию: "The output convex hull is represented as an array of indices of the contour points"
                    # Попробуем передать hull_indices как есть
                    try:
                         defects = cv2.convexityDefects(contour, hull_indices)
                    except Exception as e_defects:
                        # Иногда падает здесь, если контур и hull не совместимы
                        # print(f"[WARN] Convexity defects calculation failed: {e_defects}")
                        defects = None


                    if defects is not None:
                         # defects shape: (num_defects, 1, 4) -> [start_idx, end_idx, farthest_pt_idx, depth]
                         # Глубина (depth) - это расстояние от выпуклой оболочки до самой дальней точки контура / 256.0
                         valid_depths = [d[0][3] / 256.0 for d in defects if d[0][3] > 0] # Отбрасываем нулевые
                         defects_count = len(valid_depths)
                         if defects_count > 0:
                             avg_defect_depth = np.mean(valid_depths)
        except Exception as e_hull:
              # Ошибки могут возникать с очень маленькими или вырожденными контурами
              # print(f"[WARN] Convex hull or defect calculation error: {e_hull}")
              pass # Оставляем значения по умолчанию (0)

        features["convexity_defects_count"] = defects_count
        features["avg_defect_depth"] = avg_defect_depth # В пикселях

        # Aspect Ratio (из MinAreaRect)
        rect_width, rect_height = rect[1]
        features["aspect_ratio"] = max(rect_width, rect_height) / min(rect_width, rect_height) if min(rect_width, rect_height) > 0 else 0

        return features


    def compute_average_color(self, frame, contour, mask=None):
        """Вычисляет средний BGR цвет внутри контура."""
        # Создаем маску для контура
        contour_mask = np.zeros(frame.shape[:2], dtype=np.uint8)
        cv2.drawContours(contour_mask, [contour], -1, 255, -1) # Заливаем контур

        # Если есть общая маска рабочей зоны, объединяем их
        if mask is not None:
             final_mask = cv2.bitwise_and(contour_mask, mask)
        else:
             final_mask = contour_mask

        # Вычисляем средний цвет только по пикселям внутри финальной маски
        if cv2.countNonZero(final_mask) > 0: # Убедимся, что есть пиксели для анализа
             mean_color_bgr_a = cv2.mean(frame, mask=final_mask)
             # Возвращаем BGR кортеж (без альфа-канала, если он есть)
             return (int(mean_color_bgr_a[0]), int(mean_color_bgr_a[1]), int(mean_color_bgr_a[2]))
        else:
             # Если маска пуста (контур полностью вне рабочей зоны?)
             return (0, 0, 0) # Возвращаем черный цвет
```

**`calib.py` (Небольшие правки)**

```python
# calib.py (с небольшими правками)

import cv2
import numpy as np
import os
import time
import logging

# Используем импорт из variables с псевдонимом var
import variables as var

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def _get_charuco_board():
    """Создает и возвращает объект CharucoBoard."""
    try:
        # Убедимся, что тип словаря поддерживается OpenCV
        if not hasattr(cv2.aruco, var.DICT_TYPE):
            logging.error(f"ArUco dictionary type {var.DICT_TYPE} not found in cv2.aruco.")
            # Попытаемся использовать стандартный словарь как запасной вариант
            fallback_dict = cv2.aruco.DICT_6X6_250
            logging.warning(f"Falling back to default dictionary: {fallback_dict}")
            aruco_dict = cv2.aruco.getPredefinedDictionary(fallback_dict)
        else:
             aruco_dict = cv2.aruco.getPredefinedDictionary(var.DICT_TYPE)

        board = cv2.aruco.CharucoBoard(
             (var.SQUARES_X, var.SQUARES_Y),
             var.SQUARE_LENGTH,
             var.MARKER_LENGTH,
             aruco_dict
         )
        return board, aruco_dict
    except Exception as e:
         logging.error(f"Failed to create Charuco board: {e}")
         raise # Передаем исключение выше, т.к. без доски калибровка невозможна


def calibrate_camera_gui(source_type="video", camera_index=0, video_path=None, max_captures=50,
                         frame_callback=None, done_callback=None, stop_flag_getter=None):
    """
    Выполняет калибровку камеры Charuco, используя видеопоток или камеру.
    Передает кадры и результат через колбэки (для интеграции с GUI).

    :param source_type: "video" или "camera".
    :param camera_index: Индекс камеры (если source_type="camera").
    :param video_path: Путь к видеофайлу (если source_type="video").
    :param max_captures: Максимальное количество кадров для калибровки.
    :param frame_callback: Функция, вызываемая для каждого кадра (принимает frame).
    :param done_callback: Функция, вызываемая по завершении (принимает cameraMatrix, distCoeffs).
    :param stop_flag_getter: Функция, возвращающая True для прерывания калибровки.
    """
    logging.info(f"Starting GUI calibration. Source: {source_type}, Index/Path: {camera_index if source_type=='camera' else video_path}, Max captures: {max_captures}")

    try:
        board, aruco_dict = _get_charuco_board()
    except Exception:
         if done_callback: done_callback(None, None)
         return # Не можем продолжить без доски

    allCorners = []
    allIds = []
    imageSize = None # Определяется по первому кадру
    cap = None # Инициализируем здесь

    # --- Открытие источника видео ---
    try:
        if source_type == "video" and video_path and os.path.exists(video_path):
            cap = cv2.VideoCapture(video_path)
        elif source_type == "camera":
            cap = cv2.VideoCapture(camera_index)
            # Попытка установить разрешение (может не работать на всех камерах)
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, var.CAMERA_WIDTH)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, var.CAMERA_HEIGHT)
        else:
            error_msg = f"Invalid source type ('{source_type}') or video path ('{video_path}') does not exist."
            logging.error(error_msg)
            print(error_msg)
            if done_callback: done_callback(None, None)
            return

        if not cap or not cap.isOpened():
             error_msg = f"Could not open video source (Type: {source_type}, Index/Path: {camera_index if source_type=='camera' else video_path})."
             logging.error(error_msg)
             print(error_msg)
             if done_callback: done_callback(None, None)
             return
    except Exception as e:
        error_msg = f"Error opening video source: {e}"
        logging.error(error_msg)
        print(error_msg)
        if done_callback: done_callback(None, None)
        return


    # --- Цикл обработки кадров ---
    frame_count = 0
    saved_frames = 0
    consecutive_failures = 0
    max_consecutive_failures = 150 # Прерываем, если долго нет успехов

    # Параметры детектора ArUco
    try:
        parameters = cv2.aruco.DetectorParameters()
        # parameters.adaptiveThreshConstant = 7
        # parameters.adaptiveThreshWinSizeMin = 3
        # parameters.adaptiveThreshWinSizeMax = 23
        # parameters.adaptiveThreshWinSizeStep = 10
        parameters.minMarkerPerimeterRate = 0.01 # Важно для мелких маркеров
        # parameters.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_SUBPIX # Улучшение точности углов
    except AttributeError:
        # Для старых версий OpenCV
        parameters = cv2.aruco.DetectorParameters_create() # type: ignore

    print("[Calibration] Starting frame processing loop...")
    while True:
        # Проверка флага остановки
        if stop_flag_getter and stop_flag_getter():
            logging.info("Stop flag triggered, exiting frame loop...")
            break

        try:
            ret, frame = cap.read()
        except Exception as e:
            logging.error(f"Error reading frame: {e}")
            ret = False

        if not ret:
            logging.info("End of video or failed to read frame.")
            break

        frame_count += 1
        if frame is None or frame.size == 0:
             logging.warning(f"Frame {frame_count} is empty, skipping.")
             continue

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        if imageSize is None:
             # Получаем размер из первого успешно прочитанного кадра
             imageSize = gray.shape[::-1] # (width, height)
             logging.info(f"Image size detected: {imageSize}")

        # --- Детекция маркеров ArUco ---
        corners, ids, rejected = cv2.aruco.detectMarkers(gray, aruco_dict, parameters=parameters)

        frame_display = frame.copy() # Копия для отрисовки
        found_charuco = False

        if ids is not None and len(ids) > 0:
             # --- Интерполяция углов Charuco ---
             # minMarkers: Минимальное количество маркеров для интерполяции (не используем, даем шанс)
             try:
                ret_interp, charucoCorners, charucoIds = cv2.aruco.interpolateCornersCharuco(
                     markerCorners=corners, markerIds=ids, image=gray, board=board
                )
             except Exception as e:
                  logging.warning(f"Error during interpolateCornersCharuco: {e}")
                  ret_interp = 0 # Считаем неудачей


             if ret_interp > var.MIN_CHARUCO_CORNERS:
                 # Успешно нашли достаточно углов Charuco
                 found_charuco = True
                 consecutive_failures = 0 # Сброс счетчика неудач
                 # print(f"[DEBUG] Frame {frame_count}: Found {ret_interp} Charuco corners.")

                 # Сохраняем данные для калибровки (каждый N-й кадр)
                 # Условие frame_count % var.FRAME_INTERVAL == 0 более релевантно для видео
                 should_save = False
                 if source_type == "video" and frame_count % var.FRAME_INTERVAL == 0:
                     should_save = True
                 elif source_type == "camera": # Для камеры сохраняем чаще, но не каждый кадр
                     should_save = frame_count % 5 == 0 # Каждый 5-й кадр, например

                 if should_save and saved_frames < max_captures:
                      # Сохраняем только если есть достаточное количество углов
                     allCorners.append(charucoCorners)
                     allIds.append(charucoIds)
                     saved_frames += 1
                     logging.info(f"Frame {frame_count} captured ({saved_frames}/{max_captures}). Corners: {ret_interp}")

                 # Отрисовка найденных углов Charuco
                 cv2.aruco.drawDetectedCornersCharuco(frame_display, charucoCorners, charucoIds, (0, 255, 0))
             # else: Если углов мало, ничего не делаем и не рисуем их


             # Отрисовка найденных маркеров ArUco (всегда, если они есть)
             cv2.aruco.drawDetectedMarkers(frame_display, corners, ids)

        if not found_charuco:
             consecutive_failures += 1
             # print(f"[DEBUG] Frame {frame_count}: No sufficient Charuco corners found (Failures: {consecutive_failures}). Markers found: {len(ids) if ids is not None else 0}")


        # --- Добавление оверлея с информацией ---
        aspect_ratio = imageSize[0] / imageSize[1] if imageSize and imageSize[1] != 0 else 1.0
        text_color = (0, 255, 0) # Зеленый по умолчанию
        # Показываем красным, если давно не было успешных кадров
        if consecutive_failures > 30: text_color = (0, 0, 255)

        cv2.putText(frame_display, f"Saved frames: {saved_frames}/{max_captures}",
                    (10, frame_display.shape[0] - 40), cv2.FONT_HERSHEY_SIMPLEX,
                    0.6, text_color, 2)
        cv2.putText(frame_display, f"Aspect Ratio: {aspect_ratio:.2f}",
                    (10, frame_display.shape[0] - 15), cv2.FONT_HERSHEY_SIMPLEX,
                    0.6, text_color, 2)

        # --- Передача кадра в GUI ---
        if frame_callback:
             try:
                frame_callback(frame_display)
             except Exception as e:
                  # Ловим ошибки, если GUI окно уже закрыто, а поток еще работает
                  logging.warning(f"Error in frame_callback (GUI might be closed): {e}")
                  # Если ошибка колбэка, возможно, стоит прервать цикл?
                  # break

        # --- Условия выхода из цикла ---
        if saved_frames >= max_captures:
            logging.info(f"Reached max captures ({max_captures}). Finishing.")
            break
        if consecutive_failures >= max_consecutive_failures:
             logging.warning(f"Exceeded max consecutive frame failures ({max_consecutive_failures}). Stopping.")
             break

        # Небольшая пауза, чтобы не загружать CPU на 100% и дать GUI время
        time.sleep(0.01) # ~100 FPS max processing rate

    # --- Конец цикла обработки кадров ---
    print("[Calibration] Frame processing loop finished.")
    if cap:
         cap.release()
         logging.info("Video capture released.")


    # --- Выполнение калибровки ---
    cameraMatrix = None
    distCoeffs = None

    if not allCorners or not allIds:
        logging.warning("No corners/IDs were collected for calibration.")
    elif imageSize is None:
         logging.error("Image size was not determined. Cannot calibrate.")
    elif len(allCorners) < 5: # Требуем хотя бы несколько хороших кадров
         logging.warning(f"Too few ({len(allCorners)}) frames collected for reliable calibration.")
    else:
         print(f"[Calibration] Calibrating camera using {len(allCorners)} captured frames...")
         # Начальные приближения (могут быть улучшены, если есть информация о камере)
         initial_cameraMatrix = np.array([[max(imageSize), 0.0, imageSize[0] / 2.0],
                                         [0.0, max(imageSize), imageSize[1] / 2.0],
                                         [0.0, 0.0, 1.0]], dtype=np.float64)
         initial_distCoeffs = np.zeros((5, 1), dtype=np.float64) # 5 for basic, 8 for rational? CALIB_RATIONAL_MODEL requires 8?

         calibration_flags = cv2.CALIB_RATIONAL_MODEL # Попробуем использовать модель с 8 коэффициентами

         try:
             # !Важно: allIds и allCorners должны быть list of numpy arrays
             # Убедимся в правильном формате
             if not isinstance(allIds[0], np.ndarray): allIds = [np.array(ids) for ids in allIds]
             if not isinstance(allCorners[0], np.ndarray): allCorners = [np.array(corners) for corners in allCorners]


             ret, cameraMatrix, distCoeffs, rvecs, tvecs = cv2.aruco.calibrateCameraCharuco(
                  charucoCorners=allCorners,
                  charucoIds=allIds,
                  board=board,
                  imageSize=imageSize,
                  cameraMatrix=initial_cameraMatrix, # Передаем начальное приближение
                  distCoeffs=initial_distCoeffs,    # Передаем начальное приближение
                  flags=calibration_flags
             )

             print("\n===== Calibration Results =====")
             print(f"Reprojection Error: {ret:.4f}")
             print("Camera Matrix:")
             print(cameraMatrix)
             print("Distortion Coefficients:")
             print(distCoeffs.flatten()) # flatten для удобного просмотра

             # --- Сохранение результатов ---
             if ret > 0 and ret < 1.0 : # Считаем успешным, если ошибка < 1.0 пикселя
                try:
                    # Удаляем старый файл перед сохранением нового
                    if os.path.exists(var.CALIBRATION_FILE):
                        os.remove(var.CALIBRATION_FILE)

                    np.savez(var.CALIBRATION_FILE,
                              cameraMatrix=cameraMatrix,
                              distCoeffs=distCoeffs,
                              rvecs=np.array(rvecs, dtype=object), # Сохраняем как object array
                              tvecs=np.array(tvecs, dtype=object), # Сохраняем как object array
                              reprojectionError=ret,
                              imageSize=np.array(imageSize)) # Сохраним и размер изображения
                    logging.info(f"Calibration successful. Results saved to {var.CALIBRATION_FILE}")
                    print(f"Results saved to: {var.CALIBRATION_FILE}")
                except Exception as e:
                     logging.error(f"Error saving calibration data: {e}")
                     print(f"[ERROR] Error saving calibration data: {e}")
                     # Калибровка успешна, но сохранение не удалось - результат не None
             else:
                 logging.warning(f"Calibration finished, but reprojection error ({ret:.4f}) is high or invalid. Results not saved.")
                 print(f"[WARN] Calibration reprojection error ({ret:.4f}) is too high. Results not saved.")
                 # Сбрасываем результаты, чтобы done_callback получил None
                 cameraMatrix = None
                 distCoeffs = None


         except cv2.error as e:
              logging.error(f"OpenCV error during calibration: {e}")
              print(f"[ERROR] OpenCV error during calibrateCameraCharuco: {e}")
         except Exception as e:
              logging.error(f"Unexpected error during calibration processing: {e}")
              print(f"[ERROR] Unexpected error during calibration processing: {e}")
              import traceback
              traceback.print_exc()

    # --- Вызов финального колбэка ---
    if done_callback:
        done_callback(cameraMatrix, distCoeffs)

    logging.info("Calibration process finished.")


if __name__ == "__main__":
    # Пример запуска калибровки из командной строки (без GUI)
    print("Running calibration from command line...")
    # Для запуска требуется видеофайл или камера
    # Создадим заглушку для stop_flag_getter
    stop_flag = False
    def stop_getter(): global stop_flag; return stop_flag

    # Настройки для примера
    source = "video" # или "camera"
    video = var.CALIBRATION_VIDEO_PATH
    cam_idx = var.CAMERA_INDEX
    max_frames = 50

    def print_frame(frame):
        # Уменьшаем размер для отображения в консоли (неэффективно, но для примера)
        # frame_small = cv2.resize(frame, (640, int(640*frame.shape[0]/frame.shape[1])))
        # cv2.imshow("Calibration Test", frame_small)
        # key = cv2.waitKey(1)
        # if key == ord('q'):
        #     global stop_flag
        #     stop_flag = True
        #     cv2.destroyAllWindows()
        print(".", end='', flush=True) # Просто печатаем точку на каждый кадр


    def print_result(matrix, coeffs):
         print("\nCalibration finished.")
         if matrix is not None:
             print("Success!")
         else:
             print("Failed or stopped.")

    # Запуск калибровки
    calibrate_camera_gui(
        source_type=source,
        camera_index=cam_idx,
        video_path=video,
        max_captures=max_frames,
        frame_callback=print_frame,
        done_callback=print_result,
        stop_flag_getter=stop_getter
    )

    print("\nCommand line calibration test complete.")
```

**`robot_comm.py` (Небольшие правки)**

```python
# robot_comm.py
# Убедитесь, что pyModbusTCP установлен: pip install pyModbusTCP

from pyModbusTCP.client import ModbusClient
import logging
from datetime import datetime
import time # для паузы при реконнекте

# Настроим логгер конкретно для этого модуля
log = logging.getLogger(__name__)
log.setLevel(logging.INFO)
# Добавим обработчик, если он еще не настроен глобально
if not log.handlers:
     handler = logging.StreamHandler() # Вывод в консоль
     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
     handler.setFormatter(formatter)
     log.addHandler(handler)
     # Можно добавить FileHandler аналогично main.py


class RobotComm:
    """Класс для обмена данными с роботом по Modbus TCP."""
    def __init__(self, host="192.168.0.10", port=502, timeout=1.0, auto_reconnect=True, reconnect_delay=5):
        """
        Инициализация клиента Modbus TCP.

        :param host: IP адрес робота/сервера.
        :param port: Порт Modbus TCP (стандартный 502).
        :param timeout: Таймаут соединения/операции в секундах.
        :param auto_reconnect: Пытаться ли автоматически переподключиться при разрыве.
        :param reconnect_delay: Задержка перед попыткой переподключения (сек).
        """
        self.host = host
        self.port = port
        self.timeout = timeout
        self._client = ModbusClient(host=self.host, port=self.port,
                                   timeout=self.timeout, auto_open=False, auto_close=False)
        self._connected = False
        self.auto_reconnect = auto_reconnect
        self.reconnect_delay = reconnect_delay
        log.info(f"RobotComm initialized for {self.host}:{self.port}")

    def connect(self):
        """Устанавливает соединение с Modbus сервером."""
        if self._connected:
            log.warning("Already connected.")
            return True
        log.info(f"Attempting to connect to Modbus server at {self.host}:{self.port}...")
        try:
            self._connected = self._client.open()
            if self._connected:
                log.info("Connection successful.")
                print(f"[{datetime.now()}] ✅ Connected to Modbus server at {self.host}:{self.port}")
            else:
                log.error("Connection failed.")
                print(f"[{datetime.now()}] ❌ Failed to connect to Modbus server.")
            return self._connected
        except Exception as e:
            self._connected = False
            log.exception(f"Exception during connection attempt: {e}") # Используем log.exception для стека
            print(f"[{datetime.now()}] ❌ Exception during connection: {e}")
            return False

    def disconnect(self):
        """Разрывает соединение с Modbus сервером."""
        if not self._connected:
            log.info("Already disconnected.")
            return
        log.info("Disconnecting from Modbus server...")
        try:
             if self._client.is_open: # Проверка перед закрытием
                 self._client.close()
             self._connected = False
             log.info("Disconnected successfully.")
             print(f"[{datetime.now()}] 🔌 Disconnected from Modbus server.")
        except Exception as e:
            log.exception(f"Exception during disconnection: {e}")
            print(f"[{datetime.now()}] ⚠️ Exception during disconnection: {e}")
            self._connected = False # Считаем соединение разорванным в любом случае


    def is_connected(self):
        """Проверяет текущее состояние соединения."""
        # client.is_open может быть не всегда надежным без операции чтения/записи
        # Дополнительно проверим флаг self._connected
        # return self._connected and self._client.is_open
        return self._connected

    def send_data(self, obj_id, x_mm, y_mm, width_mm, height_mm, angle, category_code, start_register=0):
        """
        Отправляет данные об объекте в регистры Modbus.
        Выполняет одну попытку записи. Включает проверку соединения и авто-реконнект, если включено.

        :param obj_id: ID объекта (int).
        :param x_mm, y_mm: Координаты центра в мм (float).
        :param width_mm, height_mm: Размеры в мм (float).
        :param angle: Угол в градусах (float).
        :param category_code: Код категории (int).
        :param start_register: Адрес начального регистра для записи (int).
        :return: True при успешной отправке, False при ошибке.
        """
        if not self.is_connected():
            log.warning("Attempted to send data while disconnected.")
            if self.auto_reconnect:
                 log.info("Auto-reconnect enabled. Trying to reconnect...")
                 print(f"[{datetime.now()}] 🔌 Modbus connection lost. Attempting to reconnect...")
                 time.sleep(self.reconnect_delay)
                 if not self.connect():
                     log.error("Reconnect attempt failed. Cannot send data.")
                     print(f"[{datetime.now()}] ❌ Reconnect failed. Data not sent.")
                     return False
                 # Если реконнект успешен, продолжаем отправку
                 log.info("Reconnect successful. Proceeding with data send.")
            else:
                 print(f"[{datetime.now()}] 🔌 Not connected. Cannot send data.")
                 return False

        # --- Подготовка данных ---
        # Умножаем float на 100 и конвертируем в int для отправки (2 знака после запятой)
        # Убедимся, что значения влезают в 16-битный регистр (-32768 to 32767 или 0 to 65535)
        # Используем signed int для координат (могут быть отрицательными?)
        # Будем считать, что робот ожидает данные в формате "сотые доли мм", как signed int.
        try:
             # Приводим к int, умножив на 100
            # Прим: для больших значений может потребоваться отправка как 32-bit (два регистра)
            data_to_send = [
                int(obj_id) & 0xFFFF, # ID как 16-бит без знака
                int(x_mm * 100),
                int(y_mm * 100),
                int(width_mm * 100) & 0xFFFF, # Размеры как 16-бит без знака
                int(height_mm * 100) & 0xFFFF,
                int(angle * 100),      # Угол со знаком ( -XX.XX до +XX.XX )
                int(category_code) & 0xFFFF # Категория как 16-бит без знака
            ]
            # Проверка на переполнение (базовая)
            for val in data_to_send[1:]: # Проверяем значения со знаком и без
                 if not (-32768 <= val <= 65535): # Грубая проверка на 16 бит
                      log.warning(f"Value {val} might be out of 16-bit range. Clamping may occur.")
                      # Можно добавить логику ограничения (clamping) здесь при необходимости


        except ValueError as e:
            log.error(f"Invalid data format for Modbus sending: {e}. Data: {[obj_id, x_mm, y_mm, width_mm, height_mm, angle, category_code]}")
            print(f"[{datetime.now()}] ❌ Invalid data format: {e}. Data not sent.")
            return False

        # --- Отправка данных ---
        log.debug(f"Sending data to registers starting at {start_register}: {data_to_send}")
        try:
            # write_multiple_registers ожидает список int
            success = self._client.write_multiple_registers(start_register, data_to_send)

            if success:
                # log.info(f"Data sent successfully: {data_to_send}") # Уменьшим логирование при успехе
                # print(f"[{datetime.now()}] -> Data sent: ID:{obj_id}, Cat:{category_code}, Pos:({x_mm:.1f},{y_mm:.1f})")
                return True
            else:
                log.error(f"Failed to send data using write_multiple_registers. Start reg: {start_register}, Data: {data_to_send}")
                print(f"[{datetime.now()}] ❌ Failed to send data via Modbus (write failed).")
                # При неудачной записи соединение может быть разорвано
                self._connected = self._client.is_open # Обновляем статус соединения
                return False
        except Exception as e:
            # Ошибки могут быть связаны с таймаутом, разрывом соединения и т.д.
            log.exception(f"Exception during data sending: {e}")
            print(f"[{datetime.now()}] ❌ Exception during data sending: {e}")
            self._connected = False # Считаем соединение потерянным при исключении
            return False

```

**`variables.py` (Незначительные правки и добавление констант)**

```python
# variables.py

import os
import cv2 # Нужен для cv2.aruco констант

# ---- Базовые пути ----
# Корень приложения
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# Директория для данных
APP_DIR = os.path.join(BASE_DIR, "app_data")

# ---- Создание директории для данных (перенесено в main.py на старте) ---
# if not os.path.exists(APP_DIR):
#     try:
#         os.makedirs(APP_DIR)
#         print(f"Created application data directory: {APP_DIR}")
#     except OSError as e:
#         print(f"[ERROR] Could not create app_data directory: {e}")
#         # Что делать в этом случае? Возможно, выйти или работать без сохранения.

# ---- Пути к файлам конфигурации и данных ----
CALIBRATION_FILE = os.path.join(APP_DIR,"camera_calibration.npz")
LOG_FILE = os.path.join(APP_DIR, "application_log.txt") # Изменил имя лог-файла
DATA_COLLECTION_CSV = os.path.join(APP_DIR, "collected_data.csv")
IDENTIFICATION_CONFIG = os.path.join(APP_DIR, "identification_params.json")
PARAMETERS_CONFIG = os.path.join(APP_DIR, "detection_params.json")
DETECTED_OBJECTS_CSV = os.path.join(APP_DIR, "detected_objects_log.csv") # Для лога ObjectDetector

# ---- Пути к видео файлам по умолчанию ----
# Используйте необработанные строки (r"...") или двойные слеши (\\) для путей Windows
CALIBRATION_VIDEO_PATH = r'C:\path\to\your\calibration_video.mp4' # <-- ЗАМЕНИТЕ НА ВАШ ПУТЬ
WORKING_VIDEO_PATH = r"C:\path\to\your\working_video.mp4"        # <-- ЗАМЕНИТЕ НА ВАШ ПУТЬ

# ---- Настройки Камеры ----
CAMERA_INDEX = 0       # Индекс веб-камеры (0 - встроенная, 1+ - внешние)
CAMERA_WIDTH = 1280    # Желаемая ширина кадра с камеры (может не поддерживаться)
CAMERA_HEIGHT = 720    # Желаемая высота кадра с камеры
CAMERA_FPS = 30        # Желаемая частота кадров
# CAMERA_ROTATION = 0    # Поворот кадра (0, 90, 180, 270) - обработка пока не добавлена
# CAMERA_FLIP = False    # Горизонтальное отражение - обработка пока не добавлена

# ---- Параметры Калибровки Камеры (ChArUco) ----
MAX_CAPTURES = 50               # Макс. кадров для сбора данных калибровки (было 100)
FRAME_INTERVAL = 15             # Интервал кадров для захвата из видео (было 60)
DICT_TYPE = cv2.aruco.DICT_5X5_1000 # Тип словаря ArUco маркеров
SQUARES_X = 5                   # Количество квадратов по горизонтали на доске
SQUARES_Y = 7                   # Количество квадратов по вертикали на доске
SQUARE_LENGTH = 0.0304          # (МЕТРЫ!) Длина стороны квадрата на доске (30.4 мм)
MARKER_LENGTH = 0.0154          # (МЕТРЫ!) Длина стороны маркера внутри квадрата (15.4 мм)
MIN_CHARUCO_CORNERS = 10        # Мин. количество найденных углов Charuco для учета кадра (было 12)

# ---- Параметры Детекции Объектов (Значения по умолчанию) ----
# Они будут загружены/перезаписаны из PARAMETERS_CONFIG, если он существует
SCALE = 0.75             # Масштаб изображения перед обработкой (если используется)
BLUR_KERNEL = 5          # Размер ядра размытия Гаусса (нечетное!)
CANNY_LOW = 50           # Нижний порог Canny
CANNY_HIGH = 150         # Верхний порог Canny
MIN_AREA = 100           # Минимальная площадь контура (в пикселях)
MAX_AREA = 50000         # Максимальная площадь контура (в пикселях)
CONVERSION_FACTOR = 1.0  # КОЭФФИЦИЕНТ мм/пиксель (определяется калибровкой или вручную)

# --- Параметры Обнаружения Рабочей Области ---
REFERENCE_OBJECT_WIDTH_MM = 210.0 # Ширина эталонного объекта в мм (например, короткая сторона A4)
WORKING_AREA_MIN_SIZE_RATIO = 0.1 # Минимальный размер стороны рабочей области относительно кадра (10%)
WORKING_AREA_MAX_SIZE_RATIO = 0.9 # Максимальный размер стороны рабочей области относительно кадра (90%)
# ASPECT_RATIO_TOLERANCE = 0.05   # Допуск для соотношения сторон при поиске области (если используется)

# ---- Параметры Трекинга Объектов ----
MAX_LOST_FRAMES = 10       # Макс. кадров потери трека (было 5)
MAX_DISTANCE = 75          # Макс. расстояние (в ММ!) для связи трека с детекцией (было 50 пикселей!)

# ---- Конфигурация Modbus TCP ----
MODBUS_TCP_HOST = "192.168.0.10" # IP адрес робота/сервера
MODBUS_TCP_PORT = 502            # Стандартный порт Modbus TCP
MODBUS_TIMEOUT = 1.0             # Таймаут соединения/операции (секунды)
MODBUS_AUTO_RECONNECT = True     # Автоматическое переподключение
MODBUS_RECONNECT_DELAY = 5       # Пауза перед реконнектом (секунды)

# ---- Категории объектов (пример) ----
# Маппинг имен категорий на коды для отправки роботу (используется в main.py)
# OBJECT_CATEGORY_CODES = {
#     "unknown": 0,
#     "circular_white": 1,
#     "circular_pink": 2, # или 'circular_red'
#     "circular_black": 3,
#     "rhombus_white": 4, # или 'rectangle_white'
#     "rhombus_pink": 5,
#     "rhombus_black": 6,
#     # ... другие категории ...
# }
```

Не забудьте сохранить файлы с новыми именами (`main_refactored.py`, `debug_window.py`, `utils.py`) или перезаписать существующие. Также убедитесь, что пути в `variables.py` (особенно `CALIBRATION_VIDEO_PATH` и `WORKING_VIDEO_PATH`) указаны правильно для вашей системы.

Этот рефакторинг должен сделать код более организованным и понятным. Удачи с проектом!